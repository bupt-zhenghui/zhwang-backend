# 多模态图像和谐化的研究与应用

> Research and Application of Multi-modal Image Harmonization

文档链接：[http://doc.zhenghui.tech/thesis/](http://doc.zhenghui.tech/thesis)



<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202401151352467.png" alt="image-20240115135228386" style="zoom:70%;" title="ceshi" />
    图1-1 图像和谐化任务[17]
</p>



## 摘要

图像和谐化任务是图像合成领域的一个重要子任务，其目的是对合成图像的前景进行修改，使其在视觉上与背景保持一致。此前的研究将图像和谐化任务视作一个纯粹的视觉任务，基于掩码来区分前景和背景图像。然而，在实际应用中，要求用户提供掩码提高了使用门槛。基于人的自然语言表达的主观意图来区分前景和背景显得更为便捷。为此，本文在图像和谐化的研究基础之上，对多模态的图像和谐化技术开展了以下三方面的研究：

首先，本文提出了一种新的多模态图像和谐化任务。该任务利用文本提示而非掩码来区分前景和背景区域并实现图像和谐化。文本提示的引入不仅降低了图像和谐化的实际应用门槛，也扩展了传统图像和谐化任务的研究边界。为了获得该任务所需的数据，本文在图像和谐化领域中被广泛使用的iHarmony4数据集的基础上，补充了针对前景的指代性描述，构建了一个名为ReiHarmony4的新的图像和谐化数据集。

其次，本文提出了两类方法来解决给定的任务：分割-和谐化的流水线方法以及端到端方法。流水线方法基于指称表达分割前景区域，然后进行图像和谐化。在流水线方法中，本文提出了一种新的基于预训练的扩散模型的图像和谐化模型DiffHarmony，它的性能媲美此前最先进的图像和谐化模型。端到端方法在预测前景区域的同时进行图像和谐化。本文尝试并实现了多个流水线模型以及端到端模型，所提出的模型都已在新构建的ReiHarmony4数据集上进行了广泛的测试，并取得了令人满意的结果。

第三，本文设计并实现了一个多模态图像和谐化系统平台。该平台允许用户尝试不同的图像和谐化数据集和预训练模型。它还允许用户上传合成图像以及前景的指代性描述，以获得模型输出的和谐化图片。该系统是按照软件工程开发流程构建的，并经过了全面的测试。测试结果表明，该系统能有效满足用户需求，达到预期效果。

 

**关键词：图像和谐化 多模态 数据集**

 

## Abstract

The image harmonization task is an important subtask in the field of image synthesis. It refers to the process of modifying the foreground of a composite image to ensure visual consistency with the background. Prior research has approached image harmonization as a purely visual task, utilizing masks to differentiate between the foreground and background in images. However, in practical applications, asking the user to provide a mask makes it less convenient to use. It is more preferable to differentiate between foreground and background based on the user's subjective intention expressed in natural language. As a result, this paper conducts research on multi-modal image harmonization technology, focusing on three aspects based on image harmonization research:

Firstly, this paper proposes a new multi-modal image harmonization task that uses text prompts instead of masks to differentiate foreground and background areas and harmonize the image. The introduction of text prompts makes it easier to apply image harmonization practically and also broadens the research scope of traditional image harmonization tasks. To gather the necessary data for this task, the paper creates a new image harmonization dataset called ReiHarmony4 based on the widely used iHarmony4 dataset in the image harmonization field. Additionally, referential descriptions for the foreground are added to this dataset.

Secondly, two kinds of methods have been proposed to solve the given task. The first is the split-harmonized pipeline method, which segments the foreground region based on referring expression and then performs image harmonization. In this method, a new image harmonization model based on the pre-trained diffusion model, DiffHarmony, has been proposed. Its performance is comparable to the most advanced image harmonization model. The second kind is the end-to-end method, which predicts the foreground region while performing image harmonization. Several pipeline and end-to-end models have been tried and implemented in this study. The proposed models have been extensively tested on the newly constructed ReiHarmony4 dataset, and the results are satisfactory.

Thirdly, the creation and implementation of a platform for multi-modal image harmonization has been completed. This platform allows users to try out different image harmonization datasets and pre-trained models. It also enables users to upload composite images and referring expressions to obtain the model's output, a harmonized image. The system was built according to the software engineering development process and has undergone thorough testing. The results of the testing indicate that the system can effectively meet users' needs and achieve the expected results.



**KEYWORDS: image harmonization, multi-modal, dataset**



## 符号说明







## 第一章 绪论



### 1.1 研究背景与意义

图像合成是一种常见的图像编辑操作，其目的是让一张图片中的一部分作为前景粘贴到另一张图片中，形成一张合成的图像。然而，简单的拼接会导致前景与背景的差异，从而造成视觉上的不真实。这种前景与背景的差异可以被划分为外观上的不一致（比如光照上的差异）、几何上的不一致（比如物体大小的不合理）以及语义上的不一致（比如将轮船放到了马路上）[[1](#ref-1)]。针对以上三种类型的差异，衍生出了图像合成领域中的若干个子任务，包括为前景确定合适的尺寸及位置的对象放置任务（Object Placement）、解决前景与背景之间不自然的边界问题的图像融合任务（Image Blending）以及调整前景对象的光照特征使其与背景保持一致的图像和谐化任务（Image Harmonization）等等。随着信息科技的不断发展，计算机视觉（Computer Vision）领域的研究不断深入，作为该领域的一个重要分支，图像合成技术也取得了长足的进步。

作为图像合成任务中的一个重要子任务，图像和谐化任务也具有众多的研究场景，具备极强的研究价值。例如在一张合影中加入一个没有参与的人，或是足不出户云游故宫并留下一张与故宫的合影；参与线上会议时让人像和背后的虚拟背景融为一体，或是在广告领域中合成推广物品在不同场景下的状态图，这些场景中都有图像和谐化技术的用武之地。而在和谐化技术出现以前，为了解决合成图片中前景与背景的差异性问题，往往借助各类图像编辑软件（如Photoshop）进行调整，通过微调图像中的亮度、对比度、饱和度等属性使前景部分尽可能与背景一致。然而这样的方式需要专业人员花费大量的时间和精力进行调校，并不是一种高效省时的方法。早期的图像和谐化技术可以追溯到2010年的MIH[[2](#ref-2)]，这项研究将图像和谐化技术应用到AI换脸技术上，尝试将不同人的面部特征迁移到画作蒙娜丽莎的面部并保持高度的逼真。当时的方法还是从统计学的角度对前景与背景的低级颜色特征进行比对，通过色彩迁移技术修正前景，使之与背景的色彩特征能够匹配。

随着神经网络与深度学习的迅速发展，在2010年以后，越来越多的研究人员参与到图像和谐化技术的探索中，新的数据集和新的方法不断被提出，图像和谐化技术的发展也走上了快车道。一时间，基于U-Net[[3](#ref-3)]结构的卷积神经网络（Convolutional Neural Network，CNN[[10](#ref-10)]）、基于对抗思想的域验证判别器[[4](#ref-4)]、基于Retinex光照理论[[5](#ref-5)]的本征图像和谐化[[6](#ref-6)]以及引入注意力机制、自监督训练、对比学习等新技术的各种全新方法层出不穷。近期，针对图像和谐化相关领域的研究，如不和谐区域定位问题、高分辨率图像和谐化问题、视频和谐化问题也在持续的探索当中。

传统的图像和谐化任务是一个纯视觉任务，模型的输入输出均为图像特征。本文尝试从图文多模态的视角审视图像和谐化任务，进一步提高图像和谐化的效果，同时为和谐化技术赋予更多可能。多模态技术指的是利用多种不同模态数据形式对问题开展研究，常见的模态包括视觉、声音、文字。不同模态数据的形成方式有很大的区别，例如视觉特征是基于自然界存在的连续空间，文字特征则是基于人类获取的知识积累形成的一种离散信号。对多模态技术的研究最早可以追溯到20世纪50年代的视听语音识别实验，该实验验证了人的视觉和听觉存在相互影响，不同模态之间的信息是有关联性的。

随着深度学习的发展与进步，多模态技术也在同时进行着革新，越来越多有价值的多模态任务正在研究当中。例如学习图像到文本映射的图像标注任务（Image Caption）、使用文本来搜索图片的跨模态检索任务（Cross-Modal Retrieval，CMR）、给定一张图片和一个提问生成答案的视觉问答任务（Visual Question Answering，VQA）以及近期火热的文生图任务（Text-to-Image）都是多模态技术研究的代表。按照不同多模态任务的特点，多模态技术可以划分为模态表示（Representation）、融合（Fusion）、转换（Translation）、对齐（Alignment）。形如VQA任务的多模态融合任务也表明不同模态信息的融合往往能挖掘出事物更本质的特征。

综上所述，图像和谐化与多模态技术具备重要的学术与应用价值，而本文将从图文多模态的视角开展针对于图像和谐化任务的研究，并将其应用于图像编辑系统中。



### 1.2 研究现状

在深度学习技术不断发展成熟的趋势下，图像和谐化技术的研究也愈发火热。目前，相关研究已经取得了显著的成效，但仍然还有很多亟待解决的问题。本节将先介绍图像和谐化以及多模态特征融合技术的研究现状，具体阐述相关的研究工作，并对当前和谐化技术存在的问题进行分析。



#### 1.2.1 图像和谐化研究现状

从图像和谐化任务的研究发展来看，图像和谐化的研究可以分为早期的传统和谐化方法和基于深度学习的现代和谐化方法。下面将简要介绍这两类方法。

在神经网络出现以前，图像和谐化的研究主要依赖于对图像低级颜色特征的统计分析。例如2010年的MIH[[2](#ref-2)]提出将图像分解为具有多个子带（subbands）的多分辨率金字塔，并针对每个子带在前景和背景之间进行匹配。Xue[[7](#ref-7)]等通过训练一个分类器来确认前景和背景图像块的匹配度，然后对前景区域进行针对性的调整。Lalonde[[8](#ref-8)]等提出从前景和背景图片中提取颜色簇（color clusters）特征，基于颜色簇调整前景特征，使之与背景对齐。Reinhard[[11](#ref-11)]等通过调整 $L\alpha\beta$ 通道的平均值和标准偏差，以匹配两张图像的全局颜色分布，开创了简单而有效的线性变换。

随着神经网络的风靡，基于深度学习的各类研究突飞猛进，图像和谐化技术也在发展浪潮中取得了进一步的突破。在深度学习发展的初期，Zhu[[9](#ref-9)]等开始采用卷积神经网络对真实图像与合成图像进行分类，并基于该模型计算出针对合成图像的优化参数，使得修正过的合成图像的真实性分数（realism score）能逼近真实图像的分数。Tsai[[12](#ref-12)]等提出了用于图像和谐化的端到端CNN网络DIH，并利用场景分割的辅助分支来改进基本图像和谐化网络。

2019年，Cong[[13](#ref-13)]等引入了域的概念，提出了通过域验证判别器来拉近前景域和背景域，同时还构建了专门用于图像和谐化研究的数据集iharmony4，该数据集的出现也引领了更多相关技术的研究。此后，Cong[[15](#ref-15)]等在此前研究的基础上提出了域码（domain code）的概念，利用背景对前景的关键指导信息，抽取背景空间的域码然后转移到前景中。Cun[[14](ref-14)]等设计了一个额外的空间分离注意力模块来学习低层次特征的区域外观变化以实现前景与背景的协调。Guo[[6](#ref-6)]等探究了图像和谐化问题的本质，图像基于Retinex[[5](#ref-5)]光照理论分解为反射图（reflectance）和光照图（illumination），而和谐化任务的本质是保持图像的反射图（指的是物体的固有属性）不变，调整前景的光照图（指的是受环境影响的属性）与背景一致。基于上述思想，Guo等人提出了一种自编码器，将合成图像分解，其中反射图通过一致性惩罚进行协调从而保证和谐化前后的接近，光照图通过学习背景特征并将光从背景转移到前景进行协调。随后，Guo[[16](#ref-16)]等进一步提出了以Vision Transformer为主体的和谐化模型，成功将Transformer结构的网络引入了图像和谐化任务。Sofiiuk[[17](#ref-17)]等在编码器中引入高级语义特征辅助和谐化任务的进行并取得了良好的效果。Ling[[18](#ref-18)]等设计了一个即插即用的区域感知自适应模块，显式地构建背景视觉风格并将其应用于前景。Jiang[[19](#ref-19)]等考虑到和谐化数据集构造的难度较大导致数据规模对模型性能的限制，提出一种自监督的训练框架，使得模型能够利用更大规模的数据进行训练。还有近期的各类工作[[20](#ref-20), [21](#ref-21)]基于对比学习等方法同样进行了深入的探索和试验。

除了针对标准的图像和谐化的研究，近期也出现了很多与和谐化高度相关的其他方向。例如，由于在实际应用场景中可能无法预先确定前景区域的位置，Liang[[23](#ref-23)]等据此提出了不和谐区域定位问题。此外，Cun[[24](#ref-24)]等提出的不使用前景掩码的盲图像和谐化以及Cong[[22](#ref-22)]等提出的高分辨率图像和谐化也在持续的探索当中。



#### 1.2.2 多模态融合技术研究现状

在1.2.1节中提到，实际应用场景中前景掩码的信息可能很难预先确定，这使得传统的图像和谐化技术无法直接应用。据此本文提出了一种基于图文多模态融合技术的图像和谐化策略。本节将简要介绍图文多模态融合的研究现状。

多模态融合是将不同模态的信息整理成统一表示的技术。在深度学习出现以前，多模态融合按照融合时机的不同被归为三类：早期融合、后期融合以及混合融合。早期融合方法将不同模态的信息通过简单拼接的方式进行融合。后期融合方法让不同模态的信息预先经过模型的处理，然后通过统计或机器学习的方法进行模态整合。同时包括早期融合与后期融合的技术称之为混合融合。

在以神经网络为基础的深度学习出现以后，不同模态信息的融合可以发生在网络中的任意层次，因此融合时机不再是研究的重点。基于深度学习的多模态融合技术开始关注更具体的融合方式。Ren[[25](#ref-25)]等将图像表征视作一个单词拼接在文本序列的最前端，然后使用长短时记忆网络（Long Short-term Memory Networks，LSTM[[26](#ref-26)]）对混合序列进行编码，得到的输出即为融合后的多模态特征。Malinowski[[27](#ref-27)]等在Ren的基础上，将图像特征与文本序列中的每一个单词进行拼接。此后，注意力机制和Transformer在自然语言处理和计算机视觉领域取得的巨大成功使得研究人员开始基于Transformer进行多模态融合。将图像特征划分为若干个更细粒度的图像块（patch）构造成一个类似于文本序列的图像序列特征，然后通过交叉注意力模块实现不同模态特征的融合成为了当前主流的多模态融合方法，例如视觉问答任务中的MCAN[[28](#ref-28)]、DFAF[[29](#ref-29)]等。

近期，在计算机视觉领域取得突破的预训练技术也在飞速发展，其最早的应用方式是在大规模图像分类数据集上进行训练，然后将训练好的卷积模型作为下游任务的骨干模型，并在其后添加针对下游任务的特定模型，在训练下游模型的同时微调骨干模型。此后，以BERT[[30](#ref-30)]为代表的自然语言处理领域的大规模预训练模型取得了巨大的成功，其使用的“预训练-微调”范式让模型预先在大规模的文本数据集中进行训练，然后在特定的下游任务中进行微调，从而使得一个模型具备解决多个任务的能力，并在性能上超越以往的最佳模型。预训练技术在计算机视觉和自然语言处理领域的成功，让它很自然的进入了多模态领域的研究范畴。2021年，OpenAI团队提出了基于双编码器的多模态预训练模型CLIP[[31](#ref-31)]，利用自然语言处理得到的监督信号训练出一个迁移效果很好的视觉模型，其最终目的是实现文本和图像的对齐。在CLIP的训练阶段，模型的输入是图片和文字的配对，通过对比学习的方式，对齐输入中配对的图文对。在经过超大规模的训练以后，模型就具备了对齐文本和图像的能力。训练好的模型可以通过简单的处理直接运用到下游任务当中，实现即使在零样本的条件下依旧能达到和监督模型比肩的卓越性能。在CLIP出现以后，大量图文多模态任务开始将CLIP作为预训练模型应用到下游任务中，例如Wang[[32](#ref-32)]等基于预训练好的模型提出了一个端到端的指称图像分割（Referring Image Segmentation, RIS）框架，进一步训练出一个指称图像分割的多模态模型，通过设计一个视觉语言解码器，将细粒度语义信息从文本表示传播到视觉特征中的像素级表征，促进了两种模式之间的一致性，实现文本和图像像素级别的对齐。Kwon[[33](#ref-33)]等提出的CLIPStyler实现了多模态风格迁移任务，同样是以CLIP的预训练编码器为基础架构完成的工作。



### 1.3 研究内容

本文研究的核心内容是基于多模态技术的图像和谐化任务。与传统的图像和谐化任务相比，多模态图像和谐化任务利用一句针对前景的指代性描述（Referring Expression，RE）替代传统任务中的前景掩码，以解决在实际应用场景中因没有预先确定的掩码而导致传统和谐化模型失效的问题。尽管当前已经存在盲图像和谐化相关的研究，但由于难度较大，所提出的模型还处于实验阶段，其性能表现还没有达到可以直接应用的水平。相较于重新构造出精确的前景掩码，构建一条针对前景的指代性描述显然是更为方便的。本文据此提出了多模态图像和谐化这一新任务，该任务的详细定义将在本文的第三章节进行阐述。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402192117440.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图1-1 三种不同的图像和谐化任务
</p>



由于多模态图像和谐化任务涉及到文本信息，这在此前的传统图像和谐化数据集（如iharmony4）中是不存在的，因此当前并没有可以直接应用于该任务的数据集。为此，本文提出在iharmony4数据集的基础上通过人工标注的方式为合成图像的前景部分追加指代性描述，构建出一个新的多模态和谐化数据集ReiHarmony4。

进一步的，针对多模态图像和谐化任务，本文提出了分割-和谐化的流水线以及端到端的两种主要策略，每种策略都采用了多种风格的模型进行测试。对于分割-和谐化的流水线策略，本文在标准的指称图像分割模型CRIS[[32](#ref-32)]基础上进行改进，使其作为流水线策略的一阶段分割模型；本文使用Harmony Transformer[[26](#ref-16)]以及本文所提出的基于稳定扩散模型[[34](#ref-34)]的DiffHarmony模型作为二阶段的和谐化模块。对于端到端的策略，本文提出了基于CRIS和Harmony Transformer的CRISHT模型以及基于DiffHarmony的DiffReHarmony模型。充分的实验已证实本文提出的各类模型均在ReiHarmony4数据集上取得了优异的效果。

同时，本文所提出的多模态图像和谐化模型，已经应用到实际的图像编辑系统中，面向广大用户及科研人员使用。综上所述，本文的具体研究工作包含以下三个方面：

第一，本文提出了一个新的多模态图像和谐化任务，并针对该任务标注构建了一个新的数据集ReiHarmony4。该任务利用一句文本提示替代了传统图像和谐化任务中的前景掩码，使得在一些没有掩码的场景中仍然可以对合成图片进行和谐并保证和谐化的效果，从一定程度上提高了图像和谐化的便利性，同时也是对图像和谐化任务的一种补充。

第二，本文提出了两种不同策略的方法以实现多模态的图像和谐化。在分割-和谐化流水线策略中，本文设计了基于CRIS和Harmony Transformer的二阶段模型CRIS+HT，并提出首个基于扩散模型的图像和谐化模型DiffHarmony，取得了超越此前模型的性能。在端到端模型中，本文在此前实验的基础上进一步调整模型结构，整合二阶段损失，设计了CRISHT和DiffReHarmony模型，同样在测试集中有出色的表现。

第三，本文设计并实现了多模态图像和谐化系统，将上文所提出的多模态图像和谐化模型应用于实际生产场景中，使得系统具备依据文本进行图像和谐化的能力。



### 1.4 本文安排

本文的章节安排如下：

第一章，介绍本文的研究背景与意义，图像和谐化的研究现状，多模态特征融合技术的研究现状，以及本文的主要研究工作和贡献。

第二章，围绕多模态和谐化任务以及其他本文所涉及的关键技术展开。包括图像和谐化及多模态融合的关键算法，如卷积神经网络、Transformer、扩散模型等。

第三章，介绍任务的定义和数据集的建立流程。该章节将给出新的多模态任务：多模态和谐化的定义，同时将对新数据集ReiHarmony4的构建流程进行介绍和分析。

第四章，详细介绍针对多模态和谐化任务的模型设计。包括分阶段的和谐化模型以及端到端的模型的结构，给出实验的超参数设置，最后基于实验结果对实验模型进行相关分析。

第五章，介绍本文设计并实现的多模态和谐化系统。依据软件工程的开发流程和规范，本章首先对多模态和谐化系统的业务和功能性需求进行分析，接着介绍系统设计及详细的实现细节，然后介绍系统的测试环境，记录系统的测试结果，并演示系统的整体使用流程。

第六章，总结本文对多模态图像和谐化技术的研究工作，分析现有工作的不足，同时对未来的工作进行了展望。



## 第二章 基础知识

本章将介绍多模态图像和谐化任务中所涉及到的基础知识，包括通用深度学习理论与经典模型、图像生成技术理论以及图文多模态建模理论。



### 2.1 通用深度学习理论

人类最早对于人工神经网络的研究可以追溯到20世纪40年代。当时，美国的心理学研究人员就提出了一个简单的神经元模型（MP模型[[35](#ref-35)]），拉开了人工神经网络研究的序幕。20世纪八十年代，Hinton提出了用于多层感知机的反向传播算法（Back Propagation，BP）[[36](#ref-36)]，解决了多层神经网络的复杂计算问题。然而，受限于早期计算机硬件性能不足以及神经网络层数增加带来的“梯度消失”问题，神经网络的研究一度陷入困境。

2012年的ImageNet图像识别竞赛中，由Hinton的学生Alex Krizhevsky提出的基于深度神经网络的AlexNet[[37](#ref-37)]以巨大的优势一举夺魁，将ImageNet数据集的分类错误率从25%降低到了17%。此后，深度学习迅速成为相关领域的研究中心，之后的每一届ImageNet竞赛都会出现超越以往的深度神经网络模型，如2014年Google提出的GoogleNet[[38](#ref-38)]，以及2015年微软亚洲研究院何恺明提出的深度残差网络（Deep Residual Networks[[39](#ref-39)]）。2012年之后，深度卷积网络逐渐成为计算机视觉领域首选的建模方法，ResNet网络更是凭借其优越的性能被沿用至今。与此同时，深度学习在其它领域同样快速取得了建树，例如自然语言处理领域，深度学习技术几乎完全替代了SVM与决策树等传统算法。

回顾历史，深度学习取得重大突破离不开计算机性能和算力的支持，也离不开大规模数据提供的支持。时至今日，深度学习已保持了10余年高速发展的态势，最初的AlexNet也已逐渐退出了历史舞台，取而代之的是注意力模型、Transformer模型和大规模预训练等更先进的技术。本节将从卷积神经网络开始，详细介绍近年来在计算机视觉的生成领域及多模态领域取得突破的各类模型。



#### 2.1.1 卷积神经网络

卷积神经网络（Convolutional Neural Networks，CNN）是一种基于卷积运算的深度神经网络，它的核心思想是通过卷积操作和池化操作对输入数据进行特征提取和降维，然后通过全连接层进行分类及回归。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202401181401002.png" alt="image-20240115135228386" style="zoom:70%;" title="ceshi" />
    图2-1 卷积神经网络示意图[40]
</p>

卷积神经网络的主要特点包括了参数共享、稀疏连接以及层次结构。传统的全连接网络中每个神经元都连接到下一层的全部神经元，导致计算规模巨大，一旦层次略深，计算机的算力就将超过负荷。如图2-1所示，卷积网络通过卷积核（filter）实现权重共享，通过卷积操作实现对局部的特征提取，很大程度上降低了计算规模，同时让网络对图像局部高度关联的部分有更敏锐的感知。与全连接等网络类似，卷积网络也包括输入层、隐藏层及输出层，但在隐藏层的设计上，卷积网络设计得更为精细。卷积神经网络的隐藏层通常包括卷积层、池化层和全连接层。其中卷积层负责通过卷积操作进行特征提取，通过激活函数进行非线性变换；池化层负责对特征图进行下采样，进一步的降低特征图尺寸，在提高运算效率的同时最大程度上保留特征信息；全连接层一般放在网络的末端，负责将特征图映射为向量进行最后的分类或回归计算。

!!! tip "额外介绍"

    受篇幅影响，可考虑增加ResNet的介绍



2015年，来自微软亚洲研究院（Microsoft Research Asia）的研究员何凯明（Kaiming He）等人提出了ResNet[[39](#ref-39)]。ResNet的核心创新是引入了残差学习（Residual Learning）的概念，通过使用残差块（Residual Blocks）来解决深度神经网络训练过程中的梯度消失和梯度爆炸问题。在深度神经网络中，随着网络层数的增加，网络的性能通常会提升。然而，在实际应用中，当网络变得非常深时，训练过程却变得非常困难。深度增加可能导致梯度消失或梯度爆炸，使得网络很难收敛。为了解决这一问题，传统的方法是通过权重初始化、小心设计网络结构等手段来尽量保持梯度的稳定性。ResNet的关键思想是通过引入残差学习，让网络去学习残差（residual），即输入与期望输出之间的差异。在传统的网络中，学习的目标是将输入映射到期望的输出。而在ResNet中，网络的任务变为学习输入到输出的残差。如果我们定义输入为$x$，期望输出为$H(x)$，那么ResNet学习的是$F(x) = H(x) - x$，即残差。这样的设计允许梯度直接通过网络传播，减轻了梯度消失和梯度爆炸的问题。残差块的形式如下：

$$
F(x) = ReLU(w_2 \cdot ReLU(w_1x)) + w_3x
$$


ResNet的网络结构主要由堆叠的残差块组成，通过增加网络深度，可以取得更好的性能。同时，ResNet还引入了“瓶颈”结构，即采用1x1、3x3和1x1的卷积层，以减少计算量并提高网络性能。这种结构允许在深度网络中使用较小的卷积核，提高了网络的计算效率。ResNet的成功表明，通过残差学习，可以训练非常深的神经网络而不会出现梯度问题。在ResNet的基础上所提出的一系列更深的模型，如ResNet-50、ResNet-101等，在ImageNet等任务上取得了卓越的性能。

作为计算机视觉领域绝对的核心算法，卷积神经网络至今仍然在图像的特征抽取方面发挥着巨大的作用。通过学习和提取图像特征，卷积神经网络可以实现高效的图像分类、目标检测、图像生成等下游任务，在近期的图文多模态对齐研究中，卷积神经网络也屡屡作为图像的特征提取器出现在各个热门项目当中。



#### 2.1.2 U-Net网络

在2.1.1节中提到，卷积神经网络可以实现图像特征的提取，将特征转换成一维向量就可以完成简单的分类或回归任务。但是针对一些图像到图像的转换任务，例如语义分割、图像去噪以及图像和谐化任务，显然还需要额外的设计。2015年，Ronnenberger等人提出了使用编码器-解码器结构的模型来实现医学上的图像分割任务，因为模型结构类似大写英文字母“U”，因此得名U-Net[[3](#ref-3)]。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202401181614984.png" alt="image-20240115135228386" style="zoom:30%;" title="ceshi" /><br>
    图2-2 U-Net网络结构示意图[3]
</p>

在U-Net中，编码器的结构与标准的卷积神经网络类似，用来对图像特征进行抽取，得到一个更低维度的图像高级表征。解码器可以看做是与编码器对称的操作，目的是基于图像的高级表征一步一步还原成目标图像。解码器会通过上采样的方式重构图像，同时在每一层都会与对应的编码层相关联，这种类似ResNet中跳跃连接的设计使得信息能够跨层次的交流，从而极大提升模型在图像分割等生成式场景下的准确性。

U-Net网络在诞生之初就在语义分割等图像生成式任务上取得了显著的成果，而随着时间的推移，研究人员也发现U-Net网络还有很大的改进空间。此后，越来越多先进的理论和技术的出现让U-Net网络不断的更新迭代，例如Zhou[[41](#ref-41)]等提出的UNet++，通过抓取不同层次的特征，使得融合时的特征图尺度差异更小；以及通过在U-Net结构中引入注意力机制以提高模型观察全局信息的能力[[4](#ref-4), [17](#ref-17)]。时至今日，针对U-Net网络的研究仍然在不断的探索和进步当中。



#### 2.1.3 循环神经网络与LSTM

卷积神经网络的出现极大提高了计算机处理和理解图像的能力，然而CNN并不擅长处理文字、语音类的时序信号，因为时序信号往往存在着较强的时间性关联。深度学习兴起的初期，最常用的针对时序信号建模的模型为循环神经网络（Recurrent Neural Networks，RNN[[42](#ref-42)]），RNN会基于$t-1$时间片的隐节点信息计算$t$时间片的输出，其数学表达式可以表示为：


$$
h_t = \sigma(x_t\times w_{xt} + h_{t-1} \times w_{ht} + b) \tag{2-1}
$$


<p style="text-align:center">
    <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" alt="image-20240115135228386" style="zoom:25%;" title="ceshi" /><br>
    图2-3 RNN网络结构示意图[43]
</p>



与传统的神经网络相比，循环神经网络通过引入额外的时序参数$w_{ht}$使得能够基于上一时间片的信息作出决策。然而，循环神经网络存在比较明显的“长期依赖”问题（Long Term Dependencies），当神经网络在长序列上进行计算时，之前距离较远的时间片信息可能已经被覆盖了，RNN学习远距离信息的能力明显不足。为了解决这一问题，Hochreiter等提出了第一代的长短时记忆网络（Long Short Term Memory，LSTM[[26](#ref-26)]）。在深度学习的发展阶段，LSTM经过了多代的改良，形成了较为系统的理论框架，并在时序建模领域得到了广泛的应用。如图2-4所示，LSTM在传统的循环神经网络基础上，引入了多个门控单元以实现对信息的增强处理。

1. 遗忘门（forget gate）根据上一时刻的输出$h_{t-1}$和当前输入$x_t$产生一个$f_t$值，用于确定是否让上一时刻学到的信息$C_{t-1}$通过或部分通过；

   
    $$
    f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \tag{2-2}
    $$

2. 输入门（input gate）同样依据$h_{t-1}$和$x_t$来决定哪些信息需要更新，然后使用$tanh$函数生成新的候选值$\tilde{C}_t$；

   
    $$
    \begin{align}
    i_t &=\sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \tag{2-3}\\
    \tilde{C}_t &= tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \tag{2-4}
    \end{align}
    $$

3. 依据前两步的输出对上一时刻的信息$C_{t-1}$（cell state）进行更新，得到当前时刻的信息值$C_t$；

   
    $$
    C_t = f_t * C_{t-1} + i_t * \tilde{C}_t \tag{2-5}
    $$

4. 输出门（output gate）通过$sigmoid$函数得到一个初始的输出$o_t$，然后结合$C_t$得到模型当前时刻的输出$h_t$。

   
    $$
    \begin{align}
    o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \tag{2-6}\\
    h_t &= o_t * tanh(C_t) \tag{2-7}
    \end{align}
    $$

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202401182049830.png" alt="image-20240115135228386" style="zoom:25%;" title="ceshi" /><br>
    图2-4 LSTM网络结构示意图[43]
</p>

通过在循环神经网络中引入门控机制，LSTM有效缓解了RNN中存在的长期依赖问题，使得模型在长文本序列上的计算能力和理解能力有了大幅度的提升。





#### 2.1.4 注意力机制与Transformer

卷积神经网络与循环神经网络都可以用于编码一个变长的向量序列，但无论是卷积还是循环神经网络，他们本质上还是一种针对变长序列的“局部编码”。卷积神经网络通过不断增加网络的层数来实现远距离的信息传递，循环神经网络通过引入门控机制扩大信息的感知范围。由此可见，为了让模型具备记忆更远距离信息的能力，必须通过提升网络复杂度的方式来实现，然而当前机器算力的瓶颈显然限制了复杂神经网络的发展。

2015年，Bahdanau[[44](#ref-44)]等提出了最经典的注意力（attention）结构，并将其用于机器翻译任务中，取得了显著的效果。在此之后，注意力模型基本成为了自然语言处理领域的研究核心，各种各样变种的注意力模型不断被提出并得到了广泛的应用。与卷积神经网络和循环神经网络相比，注意力模型基于更小的复杂度和更少的参数实现了更优的长距离信息的记忆，即使面对很长的序列仍然可以抓住重点，不丢失重要信息。同时，attention还解决了RNN不能并行计算的问题，attention模型中每一步的运算不依赖上一步的结果，使得可以和卷积神经网络一样并行处理。

注意力模型一般可以表示为将Query（查询）与一组键值对（Key-Value）映射到输出的过程，其中的Query、Key、Value（QKV）都是向量表示。例如在翻译任务中，Query一般被视作原语言的词向量序列，Key和Value被视作目标语词向量序列，注意力模型通过计算Query和Key之间的相似性以确定Query和Value之间的注意力关系，具体过程可以表示为函数$Attention(Q,K,V)$。


$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}}V) \tag{2-8}
$$

2017年，谷歌团队提出了Transformer[[45](#ref-45)]，用全attention的结构替换了LSTM，并在翻译任务中取得了更好的成绩，Transformer也迅速成为最有效的语言编码模型。Transformer的出现对后来的研究产生了深远的影响，从自然语言领域的BERT[[30](#ref-30)]、GPT[[46](#ref-46),[47](#ref-47)]，到多模态领域的CLIP[[31](#ref-31)]，这些预训练模型的成功都离不开Transformer模型的支撑。

<p style="text-align:center">
    <img src="https://pic1.zhimg.com/80/v2-4b53b731a961ee467928619d14a5fd44_1440w.webp" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图2-5 Transformer网络结构示意图[45]
</p>

如图2-5所示，Tranformer结构是典型的Encoder-Decoder结构。其中编码器由若干个相同的层组成，每一层均由多头自注意力和前馈神经网络两个模块组成。其中，多头注意力通过$h$个不同的线性变换对QKV进行投影，然后将线性变换结果拼接而成：


$$
\begin{align}
MultiHead(Q,K,V) = Concat(head_1, \cdots, head_h)W^O \tag{2-9}\\
where \  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \tag{}
\end{align}
$$


每一层在计算中还会引入残差连接与层归一化：


$$
\begin{align}
Z^{(l)} &= LayerNorm(H^{(l-1)} + MultiHead(Q^{(l)},K^{(l)},V^{(l)})) \tag{2-10}\\
H^{(l)} &= LayerNorm(Z^{(l)} + FFN(Z^{(l)})) \tag{2-11}
\end{align}
$$


其中，$FFN(\cdot)$表示一个两层的前馈神经网络：


$$
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 \tag{2-12}
$$


Transformer解码器结构与编码器类似，但多了一个attention层。解码器将上一位置$i-1$的输出作为Query，将编码器的输出作为Key和Value以计算当前位置$i$的输出词的概率分布。

2021年，谷歌团队将Transformer进一步推广到视觉领域，提出了视觉Transformer（Vision Transformer，ViT[[48](#ref-48)]）。ViT提出将图像划分为若干个$16\times16$的块（patch），这样每一个图像块就类似于一个词向量，将图像块序列作为token输入到自注意力模型中进行训练。大量的实验表明ViT的性能可以达到和以ResNet为代表的卷积神经网络相当的水平，这也意味着Transformer成功打破了视觉与文本之间的界限，为后续图文多模态领域的突破奠定了理论基础。



### 2.2 图像生成技术理论

图像生成技术是一类利用计算机算法和模型生成新图像的技术。这些技术在计算机视觉、人工智能和图形学等领域得到广泛应用。本节将介绍图像生成领域常见的变分自编码器、生成对抗网络以及扩散模型理论。



#### 2.2.1 变分自编码器

自编码器（Autoencoder，AE[[49](#ref-49)]）是一类无监督学习的神经网络模型，其目标是学习数据的紧凑表示，通常称为编码（encoding），并通过解码（decoding）将这一表示还原为原始数据。自编码器的结构包括一个编码器和一个解码器，两者共同协作以实现数据的重构。编码器主要负责将输入数据映射到潜在表示空间。这一映射通常通过神经网络的隐藏层实现。编码器的目标是捕捉输入数据的重要特征，并将其压缩成较低维度的表示。解码器接收编码器生成的潜在表示，并试图将其还原为原始输入数据。解码器的结构对应于编码器的镜像结构，通过反向传播误差来学习如何重构输入。自编码器在降维、特征学习和生成数据等任务上表现出色。降维自编码器通过学习数据的低维表示来实现数据的压缩，而生成自编码器则可以生成与训练数据相似的新样本。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402031752841.png" alt="image-20240115135228386" style="zoom:30%;" title="ceshi" /><br>
    图2-6 VAE结构示意图[51]
</p>

变分自编码器（Variational Autoencoder，VAE[[50](#ref-50)]）是一种生成模型，属于自编码器的一种变体。与传统自编码器不同，VAE引入了概率分布的概念，使得在潜在空间中的采样更为灵活。对于输入数据$x$，VAE使用编码器生成隐变量$z$的概率分布$q(z|x)$。通常，这个分布假设为多元正态分布，由均值$\mu(x)$和方差$\sigma^2(x)$参数化，这种随机参数化使得VAE引入了隐变量的随机性，从而让模型能够生成更多样性的样本。解码器接收采样得到的潜在变量 z，并通过解码操作生成与输入数据相似的样本。解码器的输出是生成的数据的概率分布$p(x|z)$。

VAE的训练过程通过最大化变分下界（Evidence Lower Bound，ELBO）来进行。ELBO由两部分组成：数据的重构项（生成数据的概率）和隐变量的正则化项（编码器输出的概率分布与先验分布的KL散度）。通过最大化ELBO，VAE既能够重构输入数据，又能够学到潜在分布的结构。ELBO的形式如公式2-13所示，其中，第一项表示数据的重构项，第二项是隐变量的正则化项。


$$
\text{ELBO} = \mathbb{E}_{q(z | x)} [\log p(x | z)] - \text{KL}(q(z | x) || p(z)) \tag{2-13}
$$


VAE在学习具有高度结构化潜在表示的数据上表现出色，尤其在生成新样本和插值任务中。它在图像生成、文本生成等领域取得了显著的成果。VAE的变体包括条件变分自编码器（Conditional VAE）和半监督变分自编码器（Semi-supervised VAE），它们扩展了VAE的应用范围。VAE的引入使得自编码器更适用于生成模型的任务，通过引入概率性和随机性，提高了模型生成多样性和对潜在表示的抽象能力。

$$
ELBO = E_{q(z | x)} [\log p(x | z)] - KL(q(z | x) || p(z))
$$


#### 2.2.2 生成对抗网络

生成对抗网络（Generative Adversarial Networks，GAN[[53](#ref-53)]）是由Ian Goodfellow等人于2014年提出的一种创新性的生成模型，它通过对抗训练的方式，使得生成器网络能够生成与真实数据分布相似的样本。GAN在图像生成、风格迁移等领域取得了显著的成就，吸引了广泛的研究兴趣。GAN的基本原理建立在生成器（Generator）和判别器（Discriminator）两个网络的博弈过程中。生成器试图生成逼真的样本，而判别器则努力区分生成的样本和真实样本。这一博弈过程推动两个网络的不断优化，最终达到平衡，使得生成器能够生成逼真的样本。

如图2-7所示，生成对抗网络的训练首先从分布为$p_{data}(x)$的真实数据集中采样$m$个样本，表示为$\{x^{(1)}, \cdots, x^{(m)}\}$，同时从噪声先验分布$p_g(z)$中采样$m$个噪声样本，表示为$\{z^{(1)}, \cdots, z^{(m)}\}$，并使用生成器生成$m$个样本。接着采用梯度上升策略训练判别器使得判别器可以识别样本是否是真实的。循环多次训练后，以较小的梯度对生成器进行优化，使得在多次训练以后生成器可以生成判别器无法区分的样本。

<p style="text-align:center">
    <img src="https://pic4.zhimg.com/80/v2-5ca6a701d92341b8357830cc176fb8a3_1440w.webp" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图2-7 生成对抗网络结构示意图[52]
</p>

生成对抗网络的损失函数由两部分组成，分别对应生成器损失和判别器损失。在极大极小博弈中，生成器和判别器的优化目标是相互对抗的。生成器的目标是欺骗判别器，使得生成的样本越来越逼真。其损失函数通常表示为生成器输出的样本被判别器判别为真实样本的概率的负对数：


$$
\mathcal{L}_{\text{GEN}} = -\mathbb{E}_{z \sim p(z)} [\log D(G(z))] \tag{2-14}
$$


判别器的目标是正确地区分生成的样本和真实样本。判别器的损失函数由两部分组成，分别是判别生成样本为真实样本的概率和判别真实样本为真实的概率：


$$
\mathcal{L}_{\text{DIS}} = -\mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)] - \mathbb{E}_{z \sim p(z)} [\log (1 - D(G(z)))] \tag{2-15}
$$


在整个训练过程中，生成器和判别器交替更新，通过最小化各自的损失函数来达到对抗训练的效果。这一过程形成了极大极小博弈，最终达到生成器生成逼真样本，判别器无法准确区分真实和生成样本的平衡状态。



#### 2.2.3 扩散模型

近年来，人工智能生成内容（AI Generate Content，AIGC）成为了一个前沿的研究方向，生成式模型也在不断取得新的突破。除了上文提到的变分自编码器和生成对抗网络外，扩散模型（Diffusion Model）也在图像生成式任务中展现了它强大的能力。扩散模型的思想来源于热力学的一个分支，非平衡热力学（Non-equilibrium thermodynamics），其算法理论基础是通过变分推断（Variational Inference）训练参数化的马尔可夫链（Markov Chain）。近期的诸多热门项目中都有扩散模型的身影，例如Stability.ai提出的Stable Diffusion[[54](#ref-54)]、OpenAI的DALL-E 2[[55](#ref-55)]等等，在图像生成领域的很多方向上，扩散模型也展现出超越GAN的性能。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202401261445773.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图2-8 扩散模型的加噪、去噪过程[56]
</p>

扩散模型的核心思想是让一个分布通过若干步加噪过程变为另一分布。应用于图像生成任务的扩散模型一般由正向过程和反向过程组成。在正向过程中，输入$x_0$会不断混入高斯噪声，经过$T$步加噪过程后的图像$x_T$会变成一张满足标准正态分布的噪声图像。具体而言，每一步的加噪过程会基于上一时刻图像的分布$x_{t-1}$采样出一张新的图像$x_t$。加噪公式使得图像可以逐步转变为均值为0，方差为$\textbf{I}$的噪声图像。


$$
\begin{align}
q(x_t | x_{t-1}) = \mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t \textbf{I}) \tag{2-16}\\
q(x_{1:T} | x_0) = \prod_{t=1}^{T} q(x_t | x_{t-1}) = \mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t \textbf{I}) \tag{2-17}\\
\end{align}
$$


其中的$\beta_t$为第$t$步加噪过程中分布的方差。随着$t$的增大，原图像将逐渐趋近于标准高斯噪声$\mathcal{N}(0,\textbf{I})$。

在反向过程中，通过训练一个神经网络使其能够经由$T$步去噪过程将噪声图像$x_T$还原成$x_0$。如果能够逐步求出逆转后的分布$q(x_{t-1} | x_t)$，就可以从标准正态分布$\mathcal{N}(0, \textbf{I})$中还原出$x_0$。可以利用神经网络学习这一逆转分布$p_\theta(x_{t-1}|x_t)$，其中$\theta$为神经网络中的超参数。


$$
\begin{align}
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1};\mu_\theta(x_t,t),\sigma_\theta^2(x_t,t)\textbf{I}) \tag{2-17}\\
p_\theta(x_{0:T}) = p(x_T)\prod_{t=T}^{1} \mathcal{N}(x_{t-1};\mu_\theta(x_t,t),\sigma_\theta^2(x_t,t)\textbf{I}) \tag{2-18}
\end{align}
$$

通过训练习得上式中的$\mu_\theta(x_t,t)$与$\sigma_\theta(x_t,t)$，然后基于训练中给定的$x_0$，利用贝叶斯公式间接的求解$q(x_{t-1}|x_t,x_0)$

$$
q(x_{t-1}|x_t,x_0) = q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)} = q(x_t | x_{t-1}) \frac{q(x_{t-1} | x_0)}{q(x_t | x_0)} \tag{2-19}
$$


整理后得到：


$$
q(x_{t-1} | x_t, x_0) = \mathcal{N}(x_{t-1};\tilde{\mu_t}(x_t),\tilde{\beta_t} \textbf{I}) \tag{2-20}
$$


其中：


$$
\begin{align}
\tilde{\mu_t}(x_t) &= \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\overline{\alpha_t}}} \overline{\epsilon_t}) \\
\tilde{\beta_t} &= \frac{1-\overline{\alpha_{t-1}}}{1-\overline{\alpha_t}} \beta_t \approx \beta_t \\
& where \ \alpha_t = 1-\beta_t, \overline{\alpha_t} = \prod_{i=1}^{t} \alpha_i
\end{align}
$$


模型预测的$x_{t-1}$可以写成：


$$
x_{t-1}(x_t,t;\theta) = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\overline{\alpha_t}}} \epsilon_\theta (x_t,t)) + \sigma_\theta(x_t,t)z \quad z \sim \mathcal{N}(0, \textbf{I}) \tag{2-21}
$$


为了训练上式中的$\mu_\theta(x_t,t)$与$\sigma_\theta(x_t,t)$，也即学习噪声$\epsilon_\theta(x_t,t)$，一般采用极大似然估计来预测逆扩散过程的概率分布，表示为KL散度化简得到损失为：


$$
\mathcal{L}_t^{simple} = \mathbb{E}_{x_0,t,\epsilon} \left [\Vert \epsilon - \epsilon_\theta(\sqrt{\overline{\alpha_t}}x_0 + \sqrt{1-\overline{\alpha_t}}\epsilon,t) \Vert^2 \right ] \tag{2-22}
$$


训练好的模型就具备利用随机采样的噪声还原成目标图像的能力。生成式模型在具备生成逼真图像的基本功后，往往会展开更进一步的带条件的生成。这种带条件控制的生成方法又分为两大类别，即事前训练（Classifier-Free）和事后修改（Classifier-Guidance）。其中，事后修改策略相对容易实现一些，一般只需要在训练好的模型基础上增加一个额外的分类器用于引导即可，例如Liu[[57](#ref-57)]等提出的基于语义的图像生成，使得模型能够根据图片、文本来生成与之相关的内容。而事前训练方法需要对整个模型重新训练，训练过程中除了添加高斯噪声外还要添加一个条件向量。尽管需要额外的训练成本，但如果在算力充足的条件下，事前训练方法相较于事后修改有更强的细节把控能力，最近的很多引人注目的明星项目都是基于事前训练的方法完成的，例如谷歌公司提出的Imagen[[58](#ref-58)]、OpenAI的DALL-E 2[[55](#ref-55)]。



### 2.3 图文多模态建模理论

多模态建模技术是一种整合不同模态信息的技术，旨在通过结合不同模态的数据来提高系统的性能和丰富用户体验。这种技术可以在多个领域中应用，包括计算机视觉、自然语言处理、人工智能和信息检索等。本节将从预训练技术和图文多模态对齐理论两方面介绍图文多模态建模理论。



#### 2.3.1 预训练技术

预训练技术是深度学习中一种重要的训练方法，其基本思想是在大规模的未标记数据上训练模型，然后在特定任务上进行微调。这种方法的优势在于它能够学习到通用的特征表示，从而提高模型在特定任务上的性能。预训练技术的一个重要应用是迁移学习，在迁移学习中，预训练的模型通常在一个源领域上进行训练，然后在一个不同但相关的目标领域上进行微调，使得模型能够更好地适应新的任务。预训练技术还可以用于多任务学习，其中模型在一个任务上进行预训练，然后在多个相关任务上进行微调。这有助于模型学到更通用的表示，可以应用于多个任务。

受到ImageNet图像分类竞赛的影响，预训练技术最早来源于视觉领域。当时，研究人员发现将在ImageNet数据集上训练好的VGG/ResNet-50模型迁移到其它自定义的图像分类任务上，仍然可以表现出极强的能力。因此，后续的很多视觉方面的工作都将预训练好的ResNet作为骨干模型结合针对特定下游任务的其它神经网络，取得了很好的效果。随着2017年Transformer的问世，预训练技术又渐渐开始在自然语言处理领域崭露头角。例如2018年出现的BERT[[30](#ref-30)]和GPT[[59](#ref-59)]模型，均是基于Transformer架构的网络在超大规模预料上预训练得到的模型，其中BERT擅长自然语言理解相关的任务（如文本分类），GPT更擅长文本生成类的任务。当前为人所熟知的InstrcutGPT[[60](#ref-60)]、chatGPT等都是NLP中预训练技术发展的重要成果。

随着图文预训练技术的不断成熟，以及Transformer的发展带来的视觉、语言模型大一统，多模态的预训练技术应运而生。多模态预训练的目标是通过在大规模多模态数据上进行预训练，使模型能够同时处理和理解多种类型的数据，如图像、文本、语音等。这种方法可以显著提高模型在各种任务上的性能，例如图像描述生成、视觉问答、多模态检索等。2019年的ViLBERT[[61](#ref-61)]提出用双流机制分别处理图像和文本信息，然后通过共注意力层进行跨模态交互以学习双流之间的关系。在遮蔽多模态建模任务和多模态对齐任务上预训练后的模型初步具备了跨模态匹配的能力。还有此后的ImageBERT[[62](#ref-62)]、Oscar[[63](#ref-63)]、CLIP[[31](#ref-31)]等都是被广泛用于图文多模态下游任务中的预训练模型。



#### 2.3.2 CLIP与图文多模态对齐理论

给定一张图片生成关于图片的描述，或是给定一条文本生成图片，类似这样直观的多模态任务都需要让模型具备图文对齐的能力。一般会考虑在模型中引入交叉注意力（Cross Attention）模块来实现图文对齐。上文提到的自注意力主要用于处理序列数据，例如自然语言处理中的句子或文本。在自注意力机制中使用的Query、Key、Value均由输入序列经过线性变换而来，这种设计使得模型能够对序列中的不同位置进行加权，以便关注不同位置的信息，使模型具备长距离信息的记忆能力。在交叉注意力中，Query 是用于衡量一个模态中的元素与另一个模态中的元素之间关系的向量。通常，Query 来自一个模态，例如文本模态。而Key与Value一般来自于另一个模态，用于衡量不同模态之间的关系，以便在多模态场景中进行联合学习。

CLIP（Contrastive Language-Image Pre-training）[[31](#ref-31)]是由OpenAI在2021年提出的一种图文多模态预训练模型，旨在使模型能够理解和处理图像和文本之间的关系，实现真正意义上的图文对齐。CLIP使用对比学习的思想，即通过比较正样本与负样本之间的相似度来学习特征表示。在CLIP中，正样本为配对的图像和文本对，而负样本则是从大规模的图像和文本数据集中采样的其他图像和文本。模型的训练目标为让正样本的相似度高于负样本。 CLIP旨在学习一个通用的多模态表示，使得图像和文本都能在同一个表示空间中进行比较。这使得模型能够理解图像和文本之间的语义关系，从而在各种下游任务中表现出色。 由于CLIP学到的表示空间是通用的，该模型在零样本学习上表现强大。这意味着CLIP可以在看不到特定任务的训练数据的情况下，通过将任务描述转化为文本查询，进行有效的推理和分类。因其强大的泛用性，CLIP可以应用于多种任务，包括图像分类、文本检索、图像生成描述等。它在图像和文本之间的对比学习中表现出色，为许多现实世界的多模态任务提供了强大的基础。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202401281454325.png" alt="image-20240115135228386" style="zoom:40%;" title="ceshi" /><br>
    图2-9 CLIP模型结构示意图[31]
</p>

如图2-9所示，CLIP的模型结构非常简洁，由文本编码器（Text Encoder）和图像编码器（Image Encoder）组成。其中文本编码器基于Transformer结构的模型，图像编码器尝试了基于ResNet的卷积网络和基于Transformer的ViT，两种结构的性能相当。在CLIP的训练过程中，首先通过图文编码器将一批次的$N$条图文对数据分别编码为维度为$(N,d_t)$的文本特征$[T_1,\cdots,T_N]$，以及维度为$(N,d_i)$的图像特征$[I_1,\cdots,I_N]$。得到$N$条一一对应的图文特征，如$I_1$与$T_1$，$I_N$与$T_N$均为对应关系，即为正样本。除了一一对应的正样本，还得到了$N^2-N$对不对应的图文对，如$I_1$与$T_2$，不对应的图文对即为负样本。通过计算每条图文对的余弦相似度就能得到图像和文本特征的关联度，因此训练目标为最大化正样本的相似度、最小化负样本的相似度。整理成优化目标函数如下：


$$
\mathcal{L} = \sum_{i=1}^{N} \sum_{j=1}^{N} (I_i \cdot T_j)_{(i \ne j)} - \sum_{i=1}^{N} (I_i \cdot T_i) \tag{2-23}
$$


在超大规模的图文对数据集上训练完成后，CLIP模型就具备了匹配图文的能力，这种能力迁移到下游任务中即使在零样本（zero-shot）的条件下就能展现出其惊人的能力。例如在ImageNet图像分类任务中，通过将标签结合提示（prompt）转化为文本，基于CLIP的编码器抽取特征，就能实现对图像的分类。在图像分类任务中，CLIP模型在不经过任何额外训练的条件下就具备和ResNet相当的性能，而在其它几十个任务上更是远远超过了当时的SoTA模型。在CLIP模型提出后，出现了各种各样基于CLIP解决下游多模态任务的方法，例如基于文本的风格化模型CLIPstyler[[64](#ref-64)]、指称图像分割模型CRIS[[32](#ref-32)]、CLIPSeg[[65](#ref-65)]等。



## 第三章 多模态图像和谐化定义与数据集介绍

图像和谐化任务是图像合成领域中的一个重要子任务，其目标是通过对合成图片中前景部分的修改，使前景与背景在光照等方面保持一致，从而让合成图片更为逼真。传统的图像和谐化模型大多是基于合成图像及前景掩码进行训练，依靠前景掩码对图像中的前景与背景部分进行区分，通过从背景中抽取光照特征移植到前景中实现和谐化。然而在实际应用过程中可能无法预先获得前景掩码，导致传统的和谐化模型不再适用。尽管当前已经存在针对“盲图像和谐化”（单纯凭借合成图像进行和谐化）工作的相关研究，但此类研究难度较大，且尚处于理论探索阶段，无法在短期内投入应用。基于对研究现状的分析，本文从图文多模态的角度提出了一种新的图像和谐化任务，即：多模态图像和谐化。多模态图像和谐化可以基于用户给出的一句文本提示定位前景区域，进而完成图像和谐化工作。相较于需要掩码参与的传统图像和谐化任务，多模态图像和谐化任务简化了模型的输入，为无掩码场景下的和谐化提供了新的思路。

本章将首先对所提出的多模态图像和谐化任务进行定义，随后介绍本课题针对新任务所构建的数据集ReiHarmony4的构建流程及详细的数据分析。



### 3.1 多模态图像和谐化任务定义

![ih-res_副本](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202403261346665.png)

传统图像和谐化任务是给定合成图片$\tilde{I}$以及前景掩码$M$，基于和谐化模型输出和谐化后的图像$\hat{I}$，使其尽可能地接近真实图像$I$。其数学建模如下：


$$
\hat{I} = Harm(\tilde{I}, M) \tag{3-1}
$$


其中，$Harm(\cdot)$表示和谐化过程。

多模态图像和谐化任务的定义是给定合成图片$\tilde{I}$及文本描述$T$，基于多模态和谐化模型输出和谐化后的图像$\hat{I}$，使其尽可能地接近真实图像$I$。其数学建模如下：


$$
\hat{I} = MHarm(\tilde{I}, T) \tag{3-2}
$$


其中，$MHarm(\cdot)$表示多模态和谐化过程。

特别的，模型在训练阶段可以利用前景掩码$M$的信息以提高模型定位前景的能力。对于一个二阶段的图像和谐化任务，其数学建模为：


$$
\begin{align}
M_{pred} = RIS(I, T) \tag{3-3}\\
\hat{I} = Harm(\tilde{I}, M_{pred}) \tag{3-4}
\end{align}
$$


其中，$RIS(\cdot)$表示第一阶段的指称图像分割模块，$M_{pred}$表示一阶段预测出的掩码。



### 3.2 多模态图像和谐化数据集构建

由于本文研究的内容为一个多模态领域的新任务，目前暂时还没有适用于该任务的数据集。因此，在设计模型前，需要先构建一个专门针对于该任务的数据集。数据集中的每一条训练样本应该包含：合成图片、前景掩码（用于训练阶段以提升模型定位性能）、真实图片以及一句用于唯一标识前景对象的指代性描述（Referring Expression）。如前文所述，基于深度学习的和谐化模型的训练往往需要大量成对的合成图和真实图。如果对合成图的前景进行人工调整得到真实图，获取代价极高。因此，为了减少构造多模态和谐化数据集所花费的成本，本文在iHarmony4系列数据集的基础上补充了与前景部分相对应的指称表达，构建出了一个新的多模态和谐化数据集ReiHarmony4。为了构建出该数据集，本课题首先建立了指称表达数据标注平台，基于多个指称表达生成模型提前收集前景描述数据，随后召集校内人员参与数据标注工作，最后对所标注的数据进行有效性筛选与过滤，并完成了针对数据集的统计分析工作。



#### 3.2.1 生成指称表达数据

与传统图像和谐化任务相比，多模态和谐化任务需要额外的指称表达数据用于对合成图片的前景部分进行唯一标识。因此，本课题需要在iHarmony4数据集的基础上，为每一条数据标注前景的指称表达。

在进行标注任务前，本文调研了相关图文数据集的构建思路，并重点参考了在指称表达生成（Referring Expression Generation，REG）与指称表达理解（Referring Expression Comprehention，REC）任务中经常出现的RefCOCO系列数据集。RefCOCO系列的指称表达数据集是通过要求人工评估者消除COCO数据集中由边界框描述的对象中的模糊性来收集的。RefCOCO系列数据集包括RefCOCO、RefCOCO+和RefCOCOg三个子数据集，其中RefCOCO和RefCOCO+由Kazemzadeh[[66](#ref-66)]等收集。RefCOCO+中的指称表达是严格基于外观的，它们通过禁止使用基于位置的描述来实现。具体来说，RefCOCO是在基于互动游戏的环境中收集的，而RefCOCOg[[67](#ref-67)]是在非互动环境中收集的。RefCOCOg平均每个表达有8.4个单词，而RefCOCO只有3.5个单词。如前文所述，收集类似于RefCOCO这样的数据集既耗时又费力。

随着指称表达生成任务相关研究的不断进步，利用REG模型自动生成图像边界框内对象的参考描述的技术已经成熟。为了尽可能确保标注数据的文本多样性，本文分别使用了在RefCOCO、RefCOCO+和RefCOCOg数据集上训练的三个预训练的UniRef[[68](#ref-68)]模型自动生成了多模态和谐化数据集前景的指称表达数据，分别表示为reg-a、reg-b和reg-c，如图3-1 (a) 所示。除了采用预训练的指称表达生成模型以外，本文还使用了预训练的图像标注模型OFA[[69](#ref-69)]为从合成图片中裁剪出的前景区域给出相应的描述信息。基于上述4个预训练模型就可以得到4条潜在的前景指代性描述信息。



<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402031620209.png" alt="image-20240115135228386" style="zoom:100%;" title="ceshi" /><br>
    图3-1 数据采集过程。(a)为指称表达生成过程，通过3个预训练的REG模型和1个图像描述模型得到4个参考描述文本；(b)为指称表达选择的过程，标注者在标注平台上选择合理的指称表达，如果没有合理的选项，则在下面编写文本。
</p>



#### 3.2.2 建立数据标注平台

在上一节中，本课题已经为多模态和谐化数据集获取了由预训练模型自动生成的四条前景指代性描述。然而基于模型生成的结果显然会存在一些不合理的描述，因此需要人工从中筛选出合理的指称表达。由于iHarmony4的训练和测试数据有7万余条，仅靠一个或几个人员的标注是十分费时的。因此，本课题针对这一标注任务开发了一个指称表达标注平台，并召集北京邮电大学的几十余名学生参与了数据标注任务。

本课题基于Python的Flask框架设计了标注平台的后端部分，采用MongoDB作为数据存储和管理中心。标注平台的前端为一个web页面，如图3-2所示。本课题对参与标注的人员进行了统一培训，详细介绍了标注流程与规范，通过考核的人员可以参与到该项目的正式标注工作当中。具体每条样本的标注过程大致如图3-2 (b)所示，标注人员需要基于上方给定的合成图片以及前景掩码，以多选题的形式选择下方符合要求的指代性描述。下方显示的指代性描述即位上一节中由四个模型生成的文本数据，由于四条文本可能存在重复，系统会自动删除重复出现的文本，避免完全一致的描述被多次选中。

为了进一步提高标注的准确性，降低手工标注的难度，本课题利用预训练的分割模型提前对每条描述进行了预测，基于文本以及合成图片预测出前景掩码，然后将预测出的结果与真实的前景掩码进行比对，计算出二者的IoU指标。IoU指标分布在0-1之间，指标越接近1表明预测结果越准确。页面将IoU指标作为置信度显示在每条样本的右侧作为标注的参考。此外，对于少量样本中出现的没有任何一条语句有效的情况，标注平台在下方提供了一条可以手动输入描述的文本框，用于录入标注人员提供的文本描述。

<table style="border:none">
	<tr style="border:none;padding:0;margin:0">
    <td width="50%" style="border:none;padding:0;margin:0">
        <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402022037705.png" />
    </td>
    <td style="border:none;padding:0;margin:0">
        <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402022107672.png" />
    </td>
    </tr>
    <tr style="border:none;padding:0;margin:0">
      <td style="border:none;text-align:center;padding:0;margin:0">(a) 数据标注规范</td>
      <td style="border:none;text-align:center;padding:0;margin:0">(b) 数据标注样例</td>
    </tr>
    <tr style="border:none;padding:0;margin:0">
      <td colspan="2" style="padding:0;text-align:center;border:none;margin:0">图3-2 指称表达数据标注平台</td>
    </tr>
</table>



#### 3.2.3 数据清洗

在完成了全部人工标注以后，本文为原数据集中的几乎所有样本都追加了1-4条额外的前景指代性描述信息。对于在标注过程中出现的少数难以描述的样本或是前景包含多个对象的样本，可以通过输入“none”跳过该样本的标注。本文完成了iHarmony4提供的共计73146对训练和测试图像对的前景指称表达标注。按每条标注耗时1分钟计算，总标注时常超过1200个小时。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402051357704.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图3-3 ReiHarmony4数据集样本
</p>

然而，尽管经过了细致的人工标注，一些标注后的少量数据仍然存在各种问题，进而可能导致影响模型的训练效果。因此，本文进一步对数据进行了过滤。首先通过正则表达式直接过滤了少量存在不合理字符和符号的文本，然后，使用预训练的指称图像分割模型对标注的指称表达数据进行二次检查。通过输入图像和标注文本，获得预测的前景区域，以IoU为评价指标将其与标准前景掩码进行比较，确定文本质量。本文直接删除了IoU分数小于0.1的样本。对于IoU分数稍低的样本，本课题采取了进一步的人工检查和过滤。经过整理后的新数据集ReiHarmony4中的数据样本如图3-3所示。经过了指称表达数据生成、人工数据标注以及数据清洗过程后，本文完成了多模态图像和谐化数据集ReiHarmony4的构建工作。



#### 3.2.4 数据集分析

本文所构建的ReiHarmony4数据集是对iHarmony4图像和谐化数据集的补充，为前景对象添加了指称表达信息。在标注过程中，有些图像场景复杂，或者图像中存在许多与目标物体非常相似的干扰物体，难以提供准确的唯一描述。同时，考虑到人工标注的可靠性有限，本文对标注完成后的标注结果进行了进一步的筛选和过滤。基于iHarmony4数据集，本文过滤了大约7.5％的样本，从四个子数据集中得到60,770张训练图像，其中包含121,537个指称表达，以及13097张测试图像，其中包含13097个指称表达。其中每个子数据集中的训练样本数量如表3-1所示。

<table style="text-align:center">   
    <caption>表3-1 ReiHarmony4各子数据集样本数统计</caption>
    <tr>
        <th></th>
        <th>HAdobe5K</th>
        <th>HFlickr</th>
        <th>HCOCO</th>
        <th>Hday2night</th>
        <th>Total</th>
    </tr>
    <tr>
        <td>#Training</td>
        <td>37339</td>
        <td>68687</td>
        <td>15107</td>
        <td>398</td>
        <td>121531</td>
    </tr>   
	<tr>
        <td>#Test</td>
        <td>3985</td>
        <td>7429</td>
        <td>1533</td>
        <td>144</td>
        <td>13091</td>
    </tr>
</table>



此外，各子数据集的样本比例以及原数据集各图像对的描述文本数量分布如图3-4所示：

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402051423636.png" alt="image-20240115135228386" style="zoom:40%;" title="ceshi" /><br>
    图3-4 ReiHarmony4数据集的统计信息。左图为该数据集的四个子数据集中样本所占比例；右图为本课题为原iHarmony4数据集中每条样本所增加的指称表达文本的数据条数分布
</p>

图3-5展示了ReiHarmony4数据集中的部分训练样本：

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402051501069.png" alt="image-20240115135228386" style="zoom:40%;" title="ceshi" /><br>
    图3-5 ReiHarmony4数据集的训练样本示例
</p>



### 3.3 本章小结

基于对图像和谐化研究现状和潜在缺陷的分析，本章首先提出了多模态图像和谐化这一新任务，并给出了该任务的定义。由于当前还没有针对于这一任务的相关数据集，本文基于传统图像和谐化数据集iHarmony4构建出了一个新的多模态数据集ReiHarmony4。本章从前景指代性描述数据的生成、数据标注平台的搭建、数据标注过程的解释、数据清洗以及数据集分析几个部分详细阐述了该数据集的构建流程。该数据集作为第一个针对于多模态图像和谐化任务的数据集，为后续的多模态和谐化模型研究和性能评估提供数据支撑。



## 第四章 多模态图像和谐化模型设计

本章节主要介绍针对多模态图像和谐化任务所设计的各类模型，探索此任务中图片像素级表征与文本的匹配关系建模等问题。本文首先基于传统图像和谐化模型Harmony Transformer（HT）验证了将多模态特征信息引入图像和谐化任务的有效性，随后设计并实现了分阶段的多模态图像和谐化模型CRIS+HT、CRIS+DiffHarmony，以及端到端的多模态图像和谐化模型CRISHT、DiffReHarmony。特别的，本课题提出了基于扩散模型图像和谐化模型，这也是首次将扩散模型引入图像和谐化任务的工作。此外，本文针对所提出的各个模型结合多模态图像和谐化数据集ReiHarmony4进行了充分的实验，并详细介绍了相关的实验参数设置以及实验结果分析。



### 4.1 分阶段的多模态和谐化模型

分阶段的多模态图像和谐化模型将任务划分为如图4-1所示的分割、图像和谐化两个模块。在分割阶段，通过输入图片及前景指代性描述预测出前景掩码；在和谐化阶段，基于预测出的前景掩码与和合成图片获取和谐化后的图片。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402081602959.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图4-1 分阶段的多模态图像和谐化模型结构
</p>

#### 4.1.1 多模态信息有效性验证

随着多模态技术的不断发展，越来越多的任务开始尝试通过结合不同模态的信息提高决策的准确率。由于此前尚未出现多模态视角下的图像和谐化相关工作，多模态特征是否能对图像和谐化模型带来直接的性能提升还无法确定。因此本文对将多模态的特征信息引入图像和谐化模型的设想进行了测试与验证。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402071445194.png" alt="image-20240115135228386" style="zoom:80%;" title="ceshi" /><br>
    图4-2 标准Harmony Transformer结构和引入CLIP多模态信息的CLIP-HT结构
</p>

本文选取了传统图像和谐化的经典模型HT作为验证模型，其结构大致如图4-2所示。HT的输入为由合成图片及前景掩码堆叠起来的图像对，经由卷积构成的下采样网络$Enc$对图像数据进行了降维，然后重排（reshape）特征构成4096个图像块序列，使其符合ViT网络的标准输入模式并追加相应的位置掩码。通过Transformer编码器网络$TRE$抽取图像特征后，重排特征并作对称的卷积上采样就得到了和谐化后的图像。作为首个利用Transformer结构进行图像和谐化的模型，HT的设计简洁，性能优越，参数量不大，在各个方面展现出了其强大的性能，达到了当时图像和谐化技术的最高水准。为了验证引入多模态特征对和谐化模型性能的影响，本文提出了引入预训练CLIP模型的CLIP-HT，其结构如图4-2所示。在HT的基础上，CLIP-HT引入了CLIP Encoder用于抽取合成图片中截取前景部分的特征作为多模态特征，作为交叉注意力中的Query引入和谐化模型中的Transformer模块当中。经过充分的训练后，CLIP-HT模型在iharmony4的各个子数据集上均取得了超越HT的性能表现。表4-1展示了两个模型在MSE和fMSE两个指标上的性能对比。

<table style="text-align:center">   
    <caption>表4-1 Harmony Transformer与CLIP-HT指标对比</caption>
    <tr>
        <th></th>
        <th colspan="2">Harmony Transformer</th>
        <th colspan="2">CLIP-HT</th>
    </tr>
    <tr>
        <td>Sub-dataset</td>
        <td>MSE &darr;</td>
        <td>fMSE &darr;</td>
        <td>MSE &darr;</td>
        <td>fMSE &darr;</td>
    </tr>
	<tr>
        <td>HAdobe5K</td>
        <td>47.96</td>
        <td>321.14</td>
        <td>39.02</td>
        <td>289.27</td>
    </tr>
    <tr>
        <td>HCOCO</td>
        <td>20.99</td>
        <td>377.11</td>
        <td>19.48</td>
        <td>359.57</td>
    </tr>
    <tr>
        <td>HFlickr</td>
        <td>88.41</td>
        <td>617.26</td>
        <td>84.53</td>
        <td>594.32</td>
    </tr>
    <tr>
        <td>Hday2night</td>
        <td>58.14</td>
        <td>823.68</td>
        <td>53.81</td>
        <td>746.69</td>
    </tr>
    <tr>
        <td>Total</td>
        <td>37.07</td>
        <td>395.66</td>
        <td>33.75</td>
        <td>374.05</td>
    </tr>
</table>




#### 4.1.2 指称图像分割模型

在分阶段的模型中，第一阶段模型的目标与指称图像分割任务类似，即基于图片与给定的文本提示，定位到准确的前景区域并预测出前景掩码。但与标准的指称图像分割任务不同的是，在多模态图像和谐化的第一阶段，输入的图像可以是一张合成图片，合成图片中的前景与背景存在着天然的色彩与光照差异，这使得模型在基于文本进行分辨前景与背景的同时，还可以结合光照上的差异进行辅助性的判断，从而进一步提升预测的准确性。从直观上，指称图像分割任务可以通过计算文本与图像像素级特征之间的关联性找到潜在的前景位置。相较于预训练模型CLIP所实现的图文对齐能力，指称图像分割任务需要在图文对齐的基础上寻找文本与图像细粒度特征的匹配关系。在上一章节中本文已经通过实验证实了基于预训练的CLIP模型所抽取的多模态特征可以直接引入图像和谐化模型中以提高其和谐化的能力，因此本文进一步探索了基于CLIP的指称图像分割技术。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402081604314.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图4-3 分割模块示意图
</p>
本文参考了基于CLIP的分割模型CLIPSeg[[65](#ref-65)]以及CRIS[[32](#ref-32)]的图文特征融合及匹配策略，实现了多模态图像和谐化任务中的一阶段分割模型，其结构如图4-3所示。如前文所述，与标准的指称图像分割任务略有不同的是，在第一阶段的分割模型中，输入的图像为一张前景背景存在天然差异的合成图像$I_{comp}$，其维度记作$\mathbb{R}^{H \times W \times 3}$。输入文本为前景的指代性描述Expression。将图文数据分别经由预训练的CLIP模型抽取特征得到图像特征$F_i$以及文本特征$F_t$。图4-3下方的$Fusion$模块实现了图像和文本特征的融合，融合过程中使用了CLIP文本编码器输出的文本特征$F_t$，以及图像编码器采样的若干层高级图像特征$F_{v2},F_{v3},F_{v4}$，其中$F_t \in \mathbb{R}^{L \times C},F_{v2} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C_2},F_{v3} \in \mathbb{R}^{\frac{H}{16} \times \frac{W}{16} \times C_3},F_{v4} \in \mathbb{R}^{\frac{H}{32} \times \frac{W}{32} \times C_4}$。融合采用自底向上的基本思想，首先将底层的图像特征$F_{v4}$与文本特征$F_t$进行融合，通过可学习的参数将图文特征调整为同一维度，然后通过上采样操作精炼融合特征得到多模态特征$F_{m4}$：


$$
F_{m4} = Up(\sigma(F_{v4}W_{v4}) \cdot \sigma(F_tW_t)) \tag{4-1}
$$


其中，$W_{v4}$以及$W_{t}$为可学习的参数用于将图像特征和文本特征统一到同一维度，$\sigma(\cdot)$函数表示ReLU激活函数，$Up(\cdot)$表示上采样操作，得到的多模态特征$F_{m4}$的维度为$\mathbb{R}^{\frac{H}{16} \times \frac{W}{16} \times C}$。

将得到的多模态特征与上一层图像特征$F_{v3},F_{v2}$做进一步的融合，得到多模态特征$F_{m3},F_{m2}$：


$$
\begin{align}
F_{m3} = [\sigma(F_{m4}W_{m4}), \sigma(F_{v3}W_{v3})] \tag{4-2} \\
F_{m2} = [\sigma(F_{m3}W_{m3}), \sigma(AVG(F_{v2})W_{v2})] \tag{4-3}
\end{align}
$$


通过$W_{m4}，W_{v3}$等几个参数将图像特征与多模态特征对齐，然后直接将两个特征进行拼接。得到的多模态特征$F_{m3}$的维度为$\mathbb{R}^{\frac{H}{16} \times \frac{W}{16} \times C_{m3}}$，对其进行类似操作得到多模态特征$F_{m2}$。式中的$[,]$操作表示两个特征的拼接操作，$AVG(\cdot)$表示一个卷积核为$2\times2$，步长为2的平均池化操作。拼接得到的多模态融合特征$F_{m2}$的维度为$\mathbb{R}^{\frac{H}{16} \times \frac{W}{16} \times C_{m2}}$

通过以上操作得到了多层次的多模态融合特征$F_{m2},F_{m3},F_{m4}$，接着进一步通过卷积的方式对多层次的特征进行融合：


$$
F_m = Conv([F_{m2}, F_{m3}, F_{m4}]) \tag{4-4}
$$


经由卷积融合得到了融合多模态特征$F_m$，其维度为$\mathbb{R}^{\frac{H}{16} \times \frac{W}{16} \times C}$。

经过了多层次融合得到的特征已具备了进行多模态匹配的能力，但是在指称图像分割任务中需要将文本特征与图像的像素级表征进行匹配，这使得多模态特征需要对数据的空间结构有极强的感知力。为了增强多模态特征对输入数据中的空间结构的理解，本文引入了一种用于神经网络的新型层CoordConv[[70](#ref-70)]。虽然卷积神经网络在许多任务上取得了成功，但CNN并没有充分利用输入数据中的位置信息，在处理空间数据时也往往缺乏对位置信息的显式处理。CoordConv 的提出就是为了解决这个问题。CoordConv 在输入特征图中添加额外的坐标通道，这些通道包含了输入数据中每个位置的坐标信息。这些坐标信息可以是绝对坐标、相对坐标或者其他形式的位置编码。此外，CoordConv将输入特征图的坐标通道与原始的特征通道进行拼接，从而形成一个新的特征图。这样，每个位置的特征都与其对应的位置坐标信息相关联。经过拼接后的特征图被传递给后续的网络层进行训练。由于坐标信息的引入，网络可以更好地理解输入数据中的空间结构，从而提高了网络在处理空间数据时的性能。本文通过在多模态融合特征的基础上引入基于绝对位置的坐标卷积特征，使得融合特征具备更强的位置感知能力。


$$
F_v = Conv([F_m, F_{coord}]) \tag{4-5}
$$



其中的$F_{coord}$表示CoordConv特征，其维度为$\mathbb{R}^{\frac{H}{16} \times \frac{W}{16} \times 2}$，得到的融合特征$F_v$的维度为$\mathbb{R}^{\frac{H}{16} \times \frac{W}{16} \times C}$。式中的$Conv(\cdot)$表示为一个$3\times3$的卷积操作。

为了进一步匹配文本与细粒度的图像特征，本文引入了CRIS中的视觉-语言解码器模块（Vision-Language Decoder）。该模块类似于Transformer中的解码器结构，由N个相同的网络结构叠加而成，每个网络中都包含一个多头自注意力层，一个跨模态注意力（cross-attention）层和一个前馈层。在输入多模态融合特征$F_v$以及文本特征$F_t$，之前，首先为两个特征分别添加固定的位置编码。解码器首先通过自注意力层处理多模态融合特征：


$$
F_m = MHSA(LayerNorm(F_v)) + F_v \tag{4-6}
$$


得到的$F_m$即为更新的多模态融合特征，其维度与原特征保持一致。式中的$MHSA(\cdot)$表示多头自注意力操作，其计算采用了式4-7的方式。$LayerNorm(\cdot)$为在Transformer结构中最常见的层归一化操作，并结合残差连接的操作保留一定的原本特征形式。


$$
MHSA(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V \tag{4-7}
$$


多头自注意力输入的Query、Key、Value（QKV）均来源于多模态融合特征$F_v$的映射。在对融合特征作进一步抽样后，需要利用跨模态注意力层融合文本特征。在跨模态注意力的输入中，Query（Q）来源于上一层得到的融合特征$F_m$的映射，而Key（K）和Value（V）来源于全局文本特征$F_t$的映射。


$$
F_z^{'} = MHCA(LayerNorm(F_m),F_t) + F_m \tag{4-8}
$$


其中的$MHCA(\cdot)$表示多头跨模态注意力操作，具体的计算逻辑与上一层的多头自注意力层保持一致。在得到融合特征后，通过前馈神经网络对特征做一次非线性变换就得到了最终的输出。经过N层相同网络的特征抽取后输出融合了文本的视觉特征$F_z$。


$$
F_z = MLP(LayerNorm(F_z^{'})) + F_z^{'} \tag{4-9}
$$


经过以上的若干步操作得到了最终的视觉特征$F_z$和全局文本特征$F_t$，接着需要进行文本特征与视觉特征的匹配性计算。在进行计算前，需要提前将两个特征映射为相同维度的特征。


$$
\begin{align}
z_v = Up(F_z) \cdot W_v + b_v \tag{4-10}\\
z_t = F_t \cdot W_t + b_t \tag{4-11}
\end{align}
$$


映射后得到了融合视觉特征$z_v \in \mathbb{R}^{N \times D}$和文本特征$z_t \in \mathbb{R}^D$。其中$N = \frac{H}{4} \times \frac{W}{4}$，$Up(\cdot)$表示一个$4 \times$的上采样，$W_v$和$W_t$为可学习的参数。得到的视觉特征$z_v$为一个二维特征，其中每一条行向量均表示为一个按序排列的$4\times4$像素块的图像特征。将每一条图像特征与文本特征做点积计算出图文的相似性分数（similarity score），图文相似度高的区域则为前景区域，图文相似度低的即为背景区域。由此得到相应的损失函数：


$$
\begin{align}
\mathcal{L}^{i}(z_t,z_v^i) =
\begin{cases}
    -log \sigma(z_t,z_v^i),  \qquad i \in \mathcal{P} \\
    -log(1-\sigma(z_t,z_v^i)),  i \in \mathcal{N} \tag{4-12}\\
\end{cases} \\
\mathcal{L}(z_t,z_v) = \frac{1}{N}\sum_{i \in \mathcal{P} \cup \mathcal{N}} \mathcal{L}^i(z_t,z_v^i)  \tag{4-13}
\end{align}
$$


其中$\mathcal{P}$（Positive）表示图像块位于前景区域的部分，$\mathcal{N}$（Negative）表示图像块位于背景区域的部分，$\mathcal{P} \cup \mathcal{N}$表示由前景区域和背景区域合并而成的全集。$\sigma(\cdot)$表示$sigmoid$激活函数。经过训练后的网络即具备了区分前景与背景区域的能力，模型的输出为一个0-1区间内的尺寸为$\frac{H}{4} \times \frac{W}{4}$的二维向量，向量经过一个$4\times$的上采样操作后就还原为图像原本的尺寸$H \times W$，以图片形式输出即为图4-3中右侧的掩码图像。




#### 4.1.3 图像和谐化模型

针对分阶段的多模态图像和谐化模型中的图像和谐化阶段，本文尝试了多种不同的和谐化模型进行训练和测试。和谐化模块的输入为合成图像$\tilde{H}$以及基于指称图像分割模块预测出的前景区域掩码$M_{pred}$，输出为和谐化的图像$\hat{H}$。该模块与传统的图像和谐化模型存在两个方面的差异：

1. 前景掩码是由前一阶段的分割模型预测得出，而非直接输入的标准掩码数据。
2. 和谐化模块并不要求生成的背景需要与原始背景完全保持一致，只需要从光照和色调的角度使整张图片看起来更真实即可。



##### 4.1.3.1 DHT模型

本文首先使用了和谐化Transformer中的DHT模型，其基本原理建立在Retinex光照理论下，将合成图像分解为反射图（Reflection）和光照图（Illumination），然后在保持图像反射图不变的情况下，调整前景部分的光照图使之与背景的光照特征一致。DHT模型的结构大致如图4-4所示。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402191435310.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图4-4 DHT和谐化模块示意图
</p>

在DHT和谐化模块中，输入为合成图片与预测出的前景掩码拼接得到的4通道图像，经过放缩后的维度为$\mathbb{R}^{H \times W \times 4}$，其中$H = 256,W=256$。为了获取合成图片的反射图，先通过卷积神经网络$E_R$对图像进行编码得到高级图像表征，然后重塑特征，排列成ViT结构的输入特征结构，经由Transformer编码器$TRE_R$对高级图像特征进行进一步的特征提取得到中间图像特征$F_M$。然后重排中间特征并通过上采样卷积网络还原为原始图像尺寸就得到了合成图像的光照图$\hat{R}$。


$$
\begin{align}
F_M = TRE(\phi(E_R(\tilde{H}, M_{pred}))) \tag{4-14}\\
\hat{R} = D_R(\phi^{'}(F_M)) \tag{4-15}
\end{align}
$$


其中，$\phi(\cdot)$和$\phi^{'}(\cdot)$均表示对特征进行的重塑（reshape）操作。

接下来，为了获取图像的反射图，需要先对背景的光照特征进行抽取，然后通过交叉注意力模块将背景的光照特征引入前景区域中，得到修正的合成图像的光照图$\hat{I}$。具体的实现流程如下式所示：


$$
\begin{align}
F_{light} &= TRD_L(TRE_L(LP(\tilde{H}_{p}^{bg}))) \tag{4-16}\\
\hat{I} &= D_R(\phi^{'}(TRD_I(F_M, F_{light}))) \tag{4-17}
\end{align}
$$


其中的$\tilde{H}_p^{bg}$表示遮盖了前景区域的合成图像，如图4-4所示。$LP(\cdot)$表示对图像做的线性投影使之能与位置编码结合并作为Transformer编码器的输入特征。$TRE_L$与$TRD_L$分别表示Transformer编码器与解码器，通过编码器和解码器抽取光照特征$F_{light}$。接着，再利用类似Transformer中解码器结构$TRD_I$的交叉注意力模块调整中间图像特征中前景区域的光照特征，其中输入的Query来自对光照特征的线性映射，Key和Value来自对中间图像特征的线性映射。调整后的特征经过重塑和上采样，同样还原成了与原图像大小一致的图像，作为经过修正的光照图$\hat{I}$。

依据Retinex光照理论，对图像的光照图与反射图做哈达玛积运算就能还原出原始图像。最后得出了和谐化后的图像$\hat{H}$。


$$
\hat{R} = \hat{R} \odot \hat{I} \tag{4-18}
$$


其中，$\odot$表示对特征作哈达玛积，即按元素乘（element-wise product）运算。

和谐化模块的训练目标是让得到的和谐化图片$\hat{I}$与真实图像$I$尽可能地接近。为了实现这一目标，模型采用了简单的L1 Loss作为训练中的损失函数，使得和谐化模块的输出能尽可能与真实图像接近。


$$
\mathcal{L}_1 = \mathbb{E}_{\hat{I}, I} [||\hat{I} - I||_1] \tag{4-19}
$$




##### 4.1.3.2 DiffHarmony模型

除了采用DHT模型作为分阶段多模态图像和谐化任务中的和谐化模块，本文还设计了一种新的基于扩散模型的图像和谐化模型DiffHarmony。该模型遵循LDM框架，使用稳定扩散模型（Stable Diffusion）作为预训练的基准模型。下面，本文首先将阐述在预训练的扩散模型基础上而非从头开始训练像素级的图像和谐化扩散模型的动机，然后详细介绍本文提出的基于扩散模型的图像和谐化模型，即DiffHarmony。最后，我们指出了使用扩散模型进行图像图像和谐化的挑战，并介绍了一种简单的解决方案。

在形式上，传统图像和谐化任务的输入由合成图像与前景掩码组成，输出也是一张图像。因此，该任务可以被视为一个图像到图像的翻译任务。近年来，扩散模型促进了图像到图像翻译任务的快速发展。例如，Chitwan等提出了Palette[[71](#ref-71)]，这是一个条件扩散模型，它在着色、绘画、裁剪和JPEG图像恢复四个图像到图像的转换任务上建立了一个新的SoTA。Hshmat等提出了SR3+[[72](#ref-72)]，这也是一种基于扩散的模型，其在盲超分辨任务上实现了SoTA结果。然而，这些图像到图像的扩散模型是从头开始训练的，这是计算资源密集型的。Palette的训练过程将batch-size设置为1024训练了一百万步，而SR3+的训练将batch-size设置为256或者512并训练了一百五十万步。当前的扩散模型在图像到图像的翻译任务上已经获取了足够的先验知识，因此，我们在图像和谐化的扩散模型中直接引入了预训练的扩散模型。由于stable diffusion模型在大规模数据集上训练生成的图像大多是和谐的，因此在此基础上建立的图像和谐化模型的收敛速度快。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402191704556.png" alt="image-20240115135228386" style="zoom:15%;" title="ceshi" /><br>
    图4-5 DiffHarmony图像和谐化模型示意图
</p>

DiffHarmony模型的结构如图4-5所示。模型主要由$t$个去噪过程（Denoising Step）组成。在每一个去噪步中，使用变分自编码器（VAE）对合成图像进行编码，获得其潜在表示，并对掩模进行下采样。自动编码器由编码器$\mathcal{E}$和解码器$\mathcal{D}$组成。$\mathcal{E}$接受RGB图像$I_{comp}$作为输入，并将其映射到隐变量$z$，即$z = \mathcal{E}(I)$。$\mathcal{D}$基于隐变量$z$重建图像，即$I_{harm} = \mathcal{D}(z) = \mathcal{D}(\mathcal{E}(I))$。在压缩阶段，数据维度保持图像$(c,h,w)$的格式，但通道数减少，分辨率增加。压缩图像的通道数$d=4$，VAE自动编码过程中的下采样因子$f=8$。这个设置在计算速度、样本质量和训练资源消耗之间提供了一个很好的平衡。稳定扩散模型还支持文本生成图像的能力。其文本输入使用预训练的CLIP模型作为文本编码器将文本描述转换为向量表示，通过交叉注意层将其与U-Net的输出对齐，以指导图像生成过程。在我们的图像和谐化模型DiffHarmony的配置中，我们将文本信息设置为空字符串，表示在和谐化过程中没有使用文本信息以符合稳定扩散模型的结构。

由于DiffHarmony的架构与图像绘制任务中的稳定扩散模型一致，因此本文并未从头开始训练DiffHarmony。在实验中，DiffHarmony从一个开源的预训练稳定[扩散绘画模型](https://huggingface.co/runwayml/stable-diffusion-inpainting)上微调。对于文本条件部分，我们只是简单地输入一个空字符串，表示在协调过程中不使用文本信息。DiffHarmony模型的训练损失定义为：


$$
\mathcal{L}_{diff}=\mathbb{E}_{z,z^\prime, M, \epsilon, t}\left[\left\|\left(\epsilon-\epsilon_{\theta}\left(z_{t}, t, M,z^\prime\right)\right)\odot M\right\|_{2}^{2}\right] \tag{4-20}
$$


其中，$z=\mathcal{E}(I_{real}),z^\prime=\mathcal{E}(I_{comp}), \epsilon \sim \mathcal{N}(0,1)$, $\mathcal{E}$表示预训练的变分自编码器，$M$表示下采样掩码(下采样因子$f=8$)。$\odot$表示哈达玛积运算。在训练图像和谐化模型时，本文只计算图像前景区域的损失，因为保持背景内容的能力已经被封装在稳定扩散模型中。

然而，将稳定扩散模型应用于图像和谐化任务也面临着巨大的挑战，即压缩图像的自编码器引起的重构误差。扩散模型以经过KL-reg 变分自编码压缩处理的图像特征图（feature map）作为输入，使得分辨率相对于原始图像降低了1/8。换言之，如果将$256\ × 256$分辨率的图像和掩码输入到稳定扩散模型中，扩散模型将处理分辨率仅为$32\ × 32$的特征图和掩码。这使得模型很难重建图像的内容，特别是在需要复原人脸的情况下，即使它可以生成非常和谐的图像。为了解决这个问题，我们在测试阶段直接生成分辨率为$1024\ × 1024$的图像，然后将其大小调整为$256\ × 256$进行评估。尽管如此，模型实际使用的掩码分辨率仍然只有$128\ × 128$。如何更好的处理扩散模型所带来的分辨率降低问题可能将会是接下来的研究重点。



### 4.2 端到端的多模态和谐化模型

分阶段的多模态和谐化模型将任务划分成图像分割和图像和谐化两个部分，通过同时训练两个模块实现和谐化图片的输出。在端到端的多模态和谐化模型中，只需要训练一个完整模型就能直接实现多模态的图像和谐化。本节在此前研究的基础上，对相关模型进行了整合与调整，设计了由分割模型CRIS以及图像和谐化模型DHT组合而成的端到端模型CRISHT，以及经过调整后的扩散模型DiffReHarmony。



#### 4.2.1 CRISHT模型

在上一节中详细介绍的管道化模型（CRIS+HT）将多模态和谐化任务划分为指称图像分割模块和图像和谐化模块两个步骤。在端到端的模型设计中，本文通过调整模型损失，将图像分割模块的对比损失与和谐化的L1损失进行加权计算，将两个模块整合到了一个模型CRISHT当中。CRISHT模型的损失函数可以写成：


$$
\mathcal{L} = \mathbb{E}_{z_t,z_v} [\frac{1}{N}\sum_{i \in \mathcal{P} \cup \mathcal{N}} \mathcal{L}^i(z_t,z_v^i)] + \lambda \mathbb{E}_{\hat{I},I} [||\hat{I} - I||_1] \tag{4-21}
$$


式中前一项表示分割模块的对比损失，后项表示和谐化模块的L1损失。参数$\lambda$表示两个损失的加权系数。



#### 4.2.2 DiffReHarmony模型

除了CRISHT模型，本文还提出了一个基于扩散模型的多模态和谐化模型DiffReHarmony，如图4-6所示。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402201315553.png" alt="image-20240115135228386" style="zoom:70%;" title="ceshi" /><br>
    图4-6 DiffReHarmony图像和谐化模型示意图
</p>

DiffReHarmony模型与DiffHarmony图像和谐化模型类似，但为了将此前模型输入的标准前景掩码$M$修改为依据文本提示，做了一定程度的调整。首先，由于模型不再直接输入前景掩码图像，因此此前DiffHarmony模型输入中的下采样掩码被直接设置为了全1的矩阵。其次，模型的文本输入不再是空字符串，而是一个针对前景的指代性描述。为了让当前的模型同时具备预测前景和输出和谐化图像的能力，在每一个去噪步骤中，U-Net网络不仅需要负责预测噪声，还需要负责预测前景区域掩码。本文通过在U-Net的倒数第二层添加一个输出通道为1的卷积层来实现这一点，直接得到预测的前景掩码$M_{pred}$。最后，在损失函数中加入由二元交叉熵损失和IoU损失组合的分割损失项 $L_{mask}$。通过加权的方式整合DiffHarmony模型损失，就得到了DiffReHarmony模型的损失。


$$
\mathcal{L} = \mathcal{L}_{diff} + \lambda_{mask} \mathcal{L}_{mask} \tag{4-22}
$$



### 4.3 实验设置与结果分析

本文已在前两节中介绍了为多模态图像和谐化任务所设计的两类不同的实现方案与若干相应模型。本节将针对分阶段的模型以及端到端的模型实验进行介绍，对实验结果进行分析评估，并给出可视化的结果展示。



#### 4.3.1 指称图像分割模型

在多模态和谐化二阶段模型的指称图像分割阶段，本文参考基于CLIP的指称图像分割模型（CRIS），利用合成图片以及前景指代性描述生成了预测掩码。与标准的指称图像分割任务有所不同的是，本文通过在训练阶段直接引入合成图像而非真实图像，让模型不仅能通过指代性描述定位到目标对象，还能通过学习前景和背景的差异进行定位，进一步提高了分割模型的性能。表4-1列出了在从头训练模型的条件下，分别基于合成图像和真实图像进行训练的指称图像分割任务的指标数据。

<table style="text-align:center">   
    <caption>表4-2 分割阶段基于合成图像与真实图像训练的比较</caption>
    <tr>
        <th></th>
        <th>Real Image</th>
        <th>Composite Image</th>
    </tr>
    <tr>
        <td>HCOCO</td>
        <td>63.09</td>
        <td><B>68.44</B></B></td>
    </tr>   
	<tr>
        <td>HAdobe5k</td>
        <td>73.37</td>
        <td><b>81.75</b></td>
    </tr>
    <tr>
        <td>HFlickr</td>
        <td>78.42</td>
        <td><b>83.68</b></td>
    </tr>
    <tr>
        <td>Hday2night</td>
        <td>40.76</td>
        <td><b>42.41</b></td>
    </tr>
    <tr>
        <td>ReiHarmony4</td>
        <td>67.78</td>
        <td><b>74.00</b></td>
    </tr>
</table>


注：评估指标为模型输入掩码与ground-truth的交并比（IoU），表示预测图像与真实图像的重合程度。交并比在0-100范围区间，值越高表明两个图像越接近，反之则表明两个图像相似性不足。



从指标也可以清晰的看出，在iharmony4的四个子数据集上，使用合成图像进行训练和预测的结果均要显著优于使用真实图像的结果，其IoU分数基本都达到了68以上的水平。其中在HAdobe5k数据集上的提升最为明显，相较于采用真实图片训练，直接采用合成图片训练的指标提升了11.4%。在HCOCO数据集上也有8.5%的提升，而HAdobe5k和HCOCO两个数据集已占到总数据集的87%。在提升幅度相对较小的Hday2night数据集上也达到了4.0%。在整个ReiHarmony4数据集上，采用合成图片训练和预测带来的提升为9.2%，这也进一步印证了模型确实利用了前景和背景的光照性差异辅助分割的设想。

为了让分割模型具备更出色的能力，本文在预训练好的CRIS模型上利用ReiHarmony4数据集做了微调实验，经过充分的训练以后，模型的性能进一步提升，最终的分割性能指标如表4-2所示。与从头训练的模型相比，微调的CRIS模型具备了更优越的性能，在整个数据集上的表现由74.00提高到了76.60，提升幅度为3.5%。

<table style="text-align:center">   
    <caption>表4-3 微调后的分割模型指标</caption>
    <tr>
        <th>Dataset</th>
        <th>IoU&uarr;</th>
    </tr>
    <tr>
        <td>HCOCO</td>
        <td>70.04</td>
    </tr>   
	<tr>
        <td>HAdobe5k</td>
        <td>86.05</td>
    </tr>
    <tr>
        <td>HFlickr</td>
        <td>85.05</td>
    </tr>
    <tr>
        <td>Hday2night</td>
        <td>63.08</td>
    </tr>
    <tr>
        <td>ReiHarmony4</td>
        <td>76.60</td>
    </tr>
</table>




图4-6展示了指称图像分割模块的预测结果。其中，第一列为输入的合成图片；第二列展示了预测模型所使用的前景的指代性描述以及预测的结果；第三列为前景Ground-Truth。从预测结果上可以看出，对于图片中物体较少，分布较为分散的样本，本文所提出的指称图像分割模块已经可以很好的预测出前景所在的目标区域；而对于图片中存在较多对象，特别是存在和目标对象非常接近的其他物体时，模型预测的结果并不是十分的精准。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402211245467.png" alt="image-20240115135228386" style="zoom:70%;" title="ceshi" /><br>
    图4-6 指称图像分割模块的可视化结果
</p>



#### 4.3.2 图像和谐化模型DiffHarmony

本文提出了一个新的基于扩散模型的图像和谐化模型DiffHarmony，同时将该模型与几种基于深度学习的图像和谐化方法进行了比较。表4-3给出了在iHarmony4的四个子数据集之间的定量比较。所有基线的计算结果要么直接从相应的论文中获得，要么使用官方规范进行计算。总的来说，本文提出的DiffHarmony模型在所有四个子数据集的所有评估指标上都达到了最佳性能表现。值得一提的是，对于fMSE度量指标，DiffHarmony的性能大大优于所有的基线模型指标，而MSE指标的领先幅度略有不足。这主要是因为DiffHarmony缺少混合模块，并且在训练阶段掩码输入的分辨率只有从$512\times 512$的图像上下采样得到分辨率为$64 \times 64$的输出，这使得模型无法准确地利用不协调区域的边界信息，导致融合效果不佳。

<table style="text-align:center">   
    <caption>表4-3 DiffHarmony与其它模型在iharmony4数据集下的定量比较</caption>
    <tr>
        <td></td>
        <td></td>
        <td rowspan="2">DIH<br>CVPR2017</td>
        <td rowspan="2"> DoveNet<br>CVPR2020</td>
        <td rowspan="2">iSSam<br>WACV2021</td>
        <td rowspan="2">DHT<br>ICCV2021</td>
        <td rowspan="2">DHT+<br>TPAMI2022</td>
        <td rowspan="2">CDTNet<br>CVPR2022</td>
        <td rowspan="2">SCS-Co<br>CVPR2022</td>
        <td rowspan="2">DiffHarmony</td>
    </tr>
    <tr>
        <td>Dataset</td>
        <td>Metric</td>
    </tr>
    <tr>
        <td>HCOCO</td>
        <td>PSNR&uarr;<br>MSE&darr;<br>fMSE&darr;</td>
        <td>34.69<br>51.85<br>78.99</td>
        <td>35.83<br>36.72<br>551.01</td>
        <td>39.16<br>16.48<br>266.19</td>
        <td>38.76<br>16.89<br>299.30</td>
        <td>39.22<br>14.98<br>274.66</td>
        <td>39.15<br>16.25<br>-</td>
        <td>39.88<br>13.58<br>245.54</td>
        <td><b>39.91<br>12.58<br>198.58</b></td>
    </tr>
    <tr>
        <td>HAdobe5k</td>
        <td>PSNR&uarr;<br>MSE&darr;<br>fMSE&darr;</td>
        <td>32.28<br>92.65<br>593.03</td>
        <td>34.34<br>52.32<br>380.39</td>
        <td>38.08<br>21.88<br>173.96</td>
        <td>36.88<br>38.53<br>265.11</td>
        <td>37.11<br>36.83<br>242.56</td>
        <td>38.24<br>20.62<br>-</td>
        <td>38.29<br>21.01<br>165.48</td>
        <td><b>40.14<br>20.59<br>128.99</b></td>
    </tr>
    <tr>
        <td>HFlickr</td>
        <td>PSNR&uarr;<br>MSE&darr;<br>fMSE&darr;</td>
        <td>29.55<br>163.38<br>1099.13</td>
        <td>30.21<br>133.14<br>827.03</td>
        <td>33.56<br>69.67<br>443.65</td>
        <td>33.13<br>74.51<br>515.45</td>
        <td>33.55<br>67.88<br>471.06</td>
        <td>33.55<br>68.61<br>-</td>
        <td>34.22<br>55.83<br>393.72</td>
        <td><b>35.89<br>41.64<br>261.04</b></td>
    </tr>
    <tr>
        <td>Hday2night</td>
        <td>PSNR&uarr;<br>MSE&darr;<br>fMSE&darr;</td>
        <td>34.62<br>82.34<br>1129.40</td>
        <td>35.27<br>51.95<br>1075.71</td>
        <td>37.72<br>40.59<br>590.97</td>
        <td>37.10<br>53.01<br>704.42</td>
        <td>36.38<br>49.67<br>736.58</td>
        <td>37.95<br>36.72<br>-</td>
        <td>37.83<br>41.75<br>606.80</td>
        <td><b>38.41<br>23.29<br>512.81</b></td>
    </tr>
    <tr>
        <td>All</td>
        <td>PSNR&uarr;<br>MSE&darr;<br>fMSE&darr;</td>
        <td>33.41<br>76.77<br>773.18</td>
        <td>34.76<br>52.33<br>532.62</td>
        <td>38.19<br>24.44<br>264.96</td>
        <td>37.55<br>30.30<br>320.78</td>
        <td>37.94<br>27.89<br>295.56</td>
        <td>38.23<br>23.75<br>252.05</td>
        <td>38.75<br>21.33<br>248.86</td>
        <td><b>39.50<br>18.36<br>190.91</b></td>
    </tr>
</table>



图4-7给出DiffHarmony模型和DoveNet、DHT+等其它模型在iHarmony4数据集上的定性比较结果。如图所示，本文提出的的DiffHarmony可以生成比其他基线模型更和谐的图像。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402211445872.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图4-7 各和谐化模型在iharmony4测试集上的定性比较
</p>



#### 4.3.3 多模态图像和谐化模型

对于分割-协调的管道化方法，本文首先使用前文介绍的分割模型来在ReiHarmony4数据集上执行指称图像分割任务以预测出与文本对应的前景区域，然后使用DHT以及DiffHarmony模型在预测前景的基础上进行图像和谐化。由于ReiHarmony4是iHarmony4数据集的子集，因此本文直接使用了训练好的DiffHarmony及DHT模型参与测试，没有继续在ReiHarmony4数据集上进行训练。

对于端到端的多模态和谐化方法，我们提出了CRISHT模型以及DiffReHarmony模型。CRISHT模型通过组合分阶段模型中两个预训练好的模型，即CRIS以及DHT，实现了模型的端到端输出，实验的相关参数均与分阶段模型中的参数保持一致。DiffReHarmony的训练首先使用稳定扩散模型做绘画任务的预训练权重作为初始权重，并训练不含掩码损失项的DiffHarmony以获得能够生成协调图像的初始权重。DiffReHarmony训练过程的batch-size设置为32，迭代次数为30万步。前15万步的学习率设置为$1e^{-5}$，其余15万步的学习率设置为$1e^{-6}$。接着，本课题将掩码损失项$\lambda_{mask}$的权重设置为0.01，并继续训练模型6万步，batch-size设置为32。在前3万步中，学习率设置为$1e^{-6}$，在接下来的3万步中设置为$1e^{-7}$。
与DiffHarmony使用的采样策略一样，本文在训练DiffReHarmony时使用与预训练的稳定扩散模型相同的噪声调度，并在推理过程中使用dpm - solver++[[73](#ref-73)]使得在推理过程中能够仅用5步就能生成和谐化样本。

<table style="text-align:center">   
    <caption>表4-5 多模态图像和谐化各模型定量比较</caption>
    <tr>
        <td rowspan="2">Dataset</td>
        <td rowspan="2">Metric</td>
        <td rowspan="2">Composite</td>
        <td colspan="2">Pipeline</td>
        <td colspan="2">End-to-End</td>
    </tr>
    <tr>
        <td>CRIS+HT</td>
        <td>CRIS+DiffHarmony</td>
        <td>CRISHT</td>
        <td>DiffReHarmony</td>
    </tr>
    <tr>
        <td>HCOCO</td>
        <td>MSE&darr;<br>fMSE&darr;<br>IoU&uarr;</td>
        <td>83.58<br>958.72<br>-</td>
        <td>45.39<br>617.53<br>68.44</td>
        <td><b>36.91<br>447.63</b><br>68.44</td>
        <td>59.34<br>742.51<br>65.88</td>
        <td>53.01<br>582.63<br><b>74.26</b></td>
    </tr>
    <tr>
        <td>HAdobe5k</td>
        <td>MSE&darr;<br>fMSE&darr;<br>IoU&uarr;</td>
        <td>380.79<br>2024.40<br>-</td>
        <td>68.39<br>433.81<br><b>81.75</b></td>
        <td>66.29<br>317.19<br><b>81.75</b></td>
        <td>73.56<br>476.28<br>79.13</td>
        <td><b>62.73<br>277.55</b><br>78.02</td>
    </tr>
    <tr>
        <td>HFlickr</td>
        <td>MSE&darr;<br>fMSE&darr;<br>IoU&uarr;</td>
        <td>297.88<br>1462.63<br>-</td>
        <td>129.75<br>686.92<br><b>83.68</b></td>
        <td><b>101.25<br>536.51<br>83.68</b></td>
        <td>146.84<br>716.44<br>78.82</td>
        <td>140.77<br>664.81<br>70.12</td>
    </tr>
    <tr>
        <td>Hday2night</td>
        <td>MSE&darr;<br>fMSE&darr;<br>IoU&uarr;</td>
        <td>153.04<br>1223.32<br>-</td>
        <td>121.39<br>874.60<br>42.41</td>
        <td>143.65<br>1074.04<br>42.41</td>
        <td>137.95<br>992.61<br>40.16</td>
        <td><b>115.44<br>834.68<br>69.99</b></td>
    </tr>
    <tr>
        <td>ReiHarmony4</td>
        <td>MSE&darr;<br>fMSE&darr;<br>IoU&uarr;</td>
        <td>199.95<br>1345.06<br>-</td>
        <td>62.88<br>589.76<br><b>74.00</b></td>
        <td><b>54.55<br>425.28<br>74.00</b></td>
        <td>73.08<br>701.08<br>70.62</td>
        <td>66.96<br>502.24<br>73.09</td>
    </tr>
</table>




表4-5给出了分阶段的管道化方法和端到端方法在ReiHarmony4四个子数据集上的定量比较。其中的Composite列表示初始合成图像的评估指标。除了记录模型在图像和谐化评价指标（MSE和fMSE）上的表现外，本文还记录了模型在分割任务中的指标（IoU）以进一步分析模型。从表中可以看出，无论是分阶段的方法还是端到端方法，都可以极大地提高合成图像的协调性，两种方法都能有效地完成多模态图像和谐化任务。 

总体而言，对于所提出的两种方法，分阶段的方法要略优于端到端方法。特别的，与端到端方法相比，管道化方法在HCOCO和HFlickr子数据集上具有明显的优势，但在HAdobe5k和Hday2night上性能略差。从表4-4中还能观察到，在两个较大的子数据集，即HCOCO和HAdobe5k中，端到端方法在预测掩码的任务上表现更好，但在和谐化任务上的表现较差。
这是因为端到端方法中的DiffReHarmony使用单个U-Net同时执行指称图像分割和图像和谐化任务，而没有将两个任务的输出结果结合起来进行集成优化。这使得DiffReHarmony很难在两个任务之间找到最佳平衡。这可能是今后工作中需要改进的重点之一。

最后，本文在两类多模态和谐化模型的输出结果中选取了部分样本进行可视化的展示。图4-8展示了分阶段模型CRIS+HT在测试集上的预测结果。其中图上方的文本表示每个样本的前景的指代性描述，第一行显示了合成图片；第二行展示了和谐化的结果；第三行为真实图片。通过比较和谐化图片和真实图片，可以看出分阶段的多模态和谐化模型已经具备了很好的和谐化性能。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402211624644.png" alt="image-20240115135228386" style="zoom:70%;" title="ceshi" /><br>
    图4-8 分阶段的CRIS+HT模型可视化结果
</p>


图4-9展示了两个基于Diffusion结构的模型的预测样例。在第一行的例子中，管道化模型预测的前景更为精准，使得和谐化结果更加准确；而在第二行的例子中，端到端模型预测的前景更准确，也获得了更好的和谐化结果。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402211627166.png" alt="image-20240115135228386" style="zoom:70%;" title="ceshi" /><br>
    图4-9 DiffHarmony以及DiffReHarmony模型可视化结果
</p>




### 4.4 本章小结

本章主要介绍了针对多模态图像和谐化任务所设计的几种模型，包括分阶段模型CRIS+HT、DiffHarmony，以及端到端模型CRISHT、DiffReHarmony，对模型的具体设计进行了详尽的描述。同时，本章针对所提出的几种模型进行了充分的实验，实验表明几种不同实现方式的模型在经过大规模的训练后均能有效完成多模态和谐化任务。最后，本章还对分阶段模型与端到端模型的性能表现进行了对比，探讨了包括基于扩散模型在内的各种模型在处理多模态和谐化任务上的优势与不足，并针对相关问题提出了相应的改进方案。





## 第五章 图像和谐化系统设计与实现

本章主要介绍将本文所提出的各类模型部署到实际生产应用系统中的过程，实现了一个简易的图像和谐化系统。本章严格遵循软件开发流程规范，首先对图像和谐化系统的需求进行了分析，接着进行系统设计，编写构建系统的技术方案。随后进入真正的系统开发环节，完成对整个系统的搭建。最后，本文对构建的系统进行了充分的实验测试，结果表明本文所提出的图像和谐化系统能够满足用户的和谐化需求。



### 5.1 需求分析

#### 5.1.1 业务需求分析

图像合成技术作为图像编辑领域中的一个重要内容，是在人们的日常生活中被经常用到的一项技术。例如，用户可能希望在一张多人的合照上添加一位缺席的朋友；例如在疫情期间居民们希望能在足不出户的情形下留下一张云游故宫的图片，打卡美景胜地；经营线上零售店铺的卖家或许也希望能通过技术手段向用户展示所售商品在各种场景下的造型，展示商品特点，从而达到更好的推销效果。从例子中可以看到，如果细细探索，各行各业可能都有用得上图像合成技术的角落。

作为图像合成中的一个重要子任务，图像和谐化为合成图像的真实性提供了关键保障。当前的图像和谐化工作一般是由专业的摄影人员来完成，主要的实现方式是基于一些使用较为广泛的图像编辑软件（如Photoshop），通过手动地调整图像前景区域的亮度、对比度、饱和度、曝光度等各种参数，使得前景图像能尽可能与背景区域接近。然而这样的方式存在一些明显的问题：对于图像的调整需要非常专业的人员参与，需要耗费大量的时间精力。特别是当需要和谐化的图像比较多的时候，这种手工调整的方式显得非常笨重，是一种费时费力的劳动。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402281824530.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图5-1 基于人工的传统图像和谐化方式
</p>

为了简化传统的图像和谐化手段，本文将所提出的各种和谐化模型封装成后端的和谐化算法模块，使得用户可以通过在系统网页的前端界面上调用服务端的系统模块，直接实现图像和谐化。如图5-2所示，无论是基于前景掩码的图像和谐化还是基于指代性描述的多模态和谐化，都可以通过调用算法模块的方式实现在前端页面中一键和谐。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402281847603.png" alt="image-20240115135228386" style="zoom:30%;" title="ceshi" /><br>
    图5-2 基于算法模块的图像和谐化方式
</p>



#### 5.1.2 非功能性需求分析

以上介绍了图像和谐化系统的主要业务需求，本节主要介绍系统的非功能性需求，包括系统可靠性需求、性能需求、易用性和泛用性需求等。

可靠性需求：图像和谐化系统需要具备基本的可靠性。针对系统在运行过程中可能出现的各种异常情况，例如因网络连接问题导致的丢包、图像格式错误触发的报错、用户刻意发送恶意请求等情况引发的各种故障，系统都应该可以及时检测异常，触发告警，保证服务的正常运行，避免因此类问题导致服务崩溃等事故的发生。

性能需求：图像和谐化系统应当满足基本的性能要求。在系统空闲状态下，图像和谐化系统需要在3秒以内完成对上传图像的处理，返回由后端的算法模型输出的和谐化图像。为了加速模型的运算性能，需要将各类模型部署到GPU进行运算。当同一时刻有大量需要处理的图像时，受限于服务器GPU的性能，允许系统花费更长的时间进行计算。

易用性和泛用性需求：图像和谐化系统允许用户通过web形式进行访问，通过鼠标点击的方式输入图像信息，通过键盘录入文本信息，web页面的整体设置保持简洁，符合当前的主流UI设置，确保非计算机专业人员也能很快掌握系统的使用方法。同时，系统应当具备一定的泛用性以应对未来可能的修改和补充，技术人员可以较为容易的增加新的模型或系统模块。



### 5.2 系统设计

图像和谐化系统通过浏览器-服务器框架实现，整体由一个前端的web页面和一个后端的系统服务两部分组成，整体系统架构图如图5-3所示。系统的前端页面主要负责与用户的交互行为，主要包括用户的登录注册界面以及图像和谐化系统界面；系统后端主要负责用户的注册请求、登录验证、算法封装、算法调用等模块。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402282017795.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图5-3 图像和谐化系统架构图
</p>
用户在进入系统后首先会自动跳转到登录界面，用户可在此界面进行登录操作。如果事先没有账号，可单击下方的注册按钮进行注册。成功登录后，系统会进入图像和谐化系统界面。系统提供了两种不同的图像和谐化模式，即输入前景掩码的传统图像和谐化模式以及输入前景指代性描述的多模态图像和谐化模式。在传统图像和谐化模式中，用户可以上传合成图片及相应的前景掩码，自行选择系统提供的可用模型，如“DiffHarmony”、“DHT”等。选择完毕后鼠标单击下方的“一键和谐”按钮，经过短暂等待后就可以在右侧的图片框中看到和谐化的图像。在多模态图像和谐化模式下，用户可以上传合成图片，并输入一句针对前景的文本提示，选择系统提供的多模态和谐化模型，如“DiffReHarmony”、“CRISHT”，然后点击“一键和谐”按钮就能得到和谐化的图像。

系统后端主要由若干个系统模块组成，其中的核心函数调用如表5-1所示。

- 路由模块主要负责对前端的请求进行验证，过滤潜在的恶意请求，同时为不同的请求搜索相应的实现函数。
- login模块主要负责对系统注册、登录行为的验证。当新用户进行注册时，需要对输入的用户名和密码进行校验。在确认账号密码满足系统规范后，通过单向密钥加密的方式对用户输入的密码进行加密并存入数据库中，确保用户隐私数据不受威胁。在用户进行登录时，后端由于无法通过解密手段将数据库中的数据还原为原始密码，只能通过比对密文是否一致来确认输入的密码是否正确。
- 算法模块主要负责对各类不同算法的调度方式进行统一。训练好的各类模型通过统一的模块封装，统一接入GPU运算单元及其它资源。
- 调用模块整合算法模块中的所有和谐化模型，基于用户输入的图像、文本等指令，调用相应的模型进行运算。
- 数据库模块主要负责实现基于MySQL的底层数据录入与数据查询接口，通过增删改查操作为上游任务提供数据支撑。

<table style="text-align:center; border-collapse: collapse">   
    <caption>表5-1 图像和谐化系统核心函数调用</caption>
    <tr style="border-left:none; border-right: none; border-top:2px solid black">
        <td style="border-left:none; border-right: none">方法名称</td>
        <td style="border-left:none; border-right: none">所属模块</td>
        <td style="border-left:none; border-right: none">方法简介</td>
        <td style="border-left:none; border-right: none">输入参数</td>
        <td style="border-left:none; border-right: none">输出参数</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">validation(ctx)</td>
        <td style="border-left:none; border-right: none">路由模块</td>
        <td style="border-left:none; border-right: none">校验请求来源，过滤恶意请求</td>
        <td style="border-left:none; border-right: none">ctx:{"host":"xxx", "user-agent":"xxx",...}<br>ctx:请求头相关信息</td>
        <td style="border-left:none; border-right: none">status:状态信息<br>code:状态码</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">login(act,pwd)</td>
        <td style="border-left:none; border-right: none">login模块</td>
        <td style="border-left:none; border-right: none">用户登录核验函数</td>
        <td style="border-left:none; border-right: none">act:用户名<br>pwd:密码</td>
        <td style="border-left:none; border-right: none">code:状态码</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">register(act,pwd)</td>
        <td style="border-left:none; border-right: none">login模块</td>
        <td style="border-left:none; border-right: none">用户注册函数</td>
        <td style="border-left:none; border-right: none">act:用户名<br>pwd:密码</td>
        <td style="border-left:none; border-right: none">code:状态码</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">harmony(c_img,mask,m_id)</td>
        <td style="border-left:none; border-right: none">调用模块</td>
        <td style="border-left:none; border-right: none">图像和谐化模型调用函数</td>
        <td style="border-left:none; border-right: none">c_img:合成图像输入<br>mask:掩码输入<br>m_id:模型id</td>
        <td style="border-left:none; border-right: none">status:状态信息<br>code:状态码<br>h_img:和谐化输出图像</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">mm_harmony(c_img,text,m_id)</td>
        <td style="border-left:none; border-right: none">调用模块</td>
        <td style="border-left:none; border-right: none">多模态图像和谐化模型调用函数</td>
        <td style="border-left:none; border-right: none">c_img:合成图像输入<br>text:文本输入<br>m_id:模型id</td>
        <td style="border-left:none; border-right: none">status:状态信息<br>code:状态码<br>h_img:和谐化输出图像</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">load(model,path,gpu_id)</td>
        <td style="border-left:none; border-right: none">算法模块</td>
        <td style="border-left:none; border-right: none">模型载入函数</td>
        <td style="border-left:none; border-right: none">model:模型<br>path:模型存储路径<br>gpu_id:GPU编号</td>
        <td style="border-left:none; border-right: none">model:加载权重的模型</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">insert(conn,sql)</td>
        <td style="border-left:none; border-right: none">数据库模块</td>
        <td style="border-left:none; border-right: none">数据库插入函数</td>
        <td style="border-left:none; border-right: none">conn:数据库连接单元<br>sql:插入SQL语句</td>
        <td style="border-left:none; border-right: none">status:状态信息<br>code:状态码</td>
    </tr>
    <tr style="border-left:none; border-right: none; border-bottom: 2px solid black">
        <td style="border-left:none; border-right: none">query(conn,sql)</td>
        <td style="border-left:none; border-right: none">数据库模块</td>
        <td style="border-left:none; border-right: none">数据库查询函数</td>
        <td style="border-left:none; border-right: none">conn:数据库连接单元<br>sql:查询SQL语句</td>
        <td style="border-left:none; border-right: none">status:状态信息<br>code:状态码<br>res:查询结果</td>
    </tr>
</table>






### 5.3 系统实现

本节主要介绍项目使用的主要工具和框架。系统前端主要基于Ant Design的React版本实现，后端服务基于Python的Flask框架实现。

React是一个由Facebook开发并开源的用于构建用户界面的JavaScript库。它是一个基于组件化和声明式编程的框架，旨在提高开发者构建大型、高性能、可维护的Web应用的效率。React将用户界面分解为独立且可复用的组件，每个组件都有自己的状态（state）和属性（props），可以相互嵌套和组合，使得代码更易于理解和维护。React采用声明式编程范式，开发者只需要关注界面的状态随时间如何变化，而不需要手动操作DOM元素。这种方式简化了界面的操作和更新，提高了代码的可读性和可维护性。总的来说，React是一个强大、灵活且高效的JavaScript库，其组件化、声明式、虚拟DOM等特性使得React成为了前端开发中的主流框架之一。

Ant Design是一个基于React的UI组件库，由蚂蚁金服开发和维护。它提供了一套丰富的高质量UI组件，涵盖了从基础的按钮、表单、布局到复杂的表格、图表等各种组件，以及一些特色的企业级组件，比如上传文件、日期选择器等。Ant Design提供了美观、现代的UI设计，各种组件的风格统一、清晰，易于使用和定制。它的设计语言遵循了Material Design的设计原则，使得用户可以快速构建出符合现代 UI 设计标准的应用。作为一个基于React的组件库，Ant Design与React生态系统完美整合，可以与 React Router、Redux 等常用的React库无缝配合，使得开发者能够更高效地构建出复杂的单页应用。

Flask 是一个轻量级的Python Web框架，由Armin Ronacher在2010年创建并开源。它以简洁、灵活和易用为设计理念，旨在帮助开发者快速构建Web应用。Flask的设计简洁明了，API简单易懂，学习成本低。相比于其他的Web框架，Flask更加轻量级，提供了最基本的功能，但却非常灵活，可以根据项目的需求进行扩展和定制。Flask内置了Jinja2模板引擎，使得在HTML文件中嵌入Python代码变得非常简单。Jinja2提供了强大的模板继承、条件控制、循环和过滤器等功能，使得前端开发更加高效。



在构建好图像和谐化系统后，本文对该系统进行了测试，系统测试环境如表5-2所示。

<table style="text-align:center; border-collapse: collapse">   
    <caption>表5-2 系统测试环境介绍</caption>
    <tr style="border-left:none; border-right: none; border-top:2px solid black">
        <td style="border-left:none; border-right: none">环境</td>
        <td style="border-left:none; border-right: none">服务器</td>
        <td style="border-left:none; border-right: none">客户端</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">硬件环境</td>
        <td style="border-left:none; border-right: none">CPU:Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz<br>GPU:4x NVIDIA GeForce RTX 3090<br>内存:125GB</td>
        <td style="border-left:none; border-right: none">CPU:Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz</td>
    </tr>
    <tr style="border-left:none; border-right: none">
        <td style="border-left:none; border-right: none">操作系统</td>
        <td style="border-left:none; border-right: none">Ubuntu 16.04.7 LTS</td>
        <td style="border-left:none; border-right: none">MacOS 14.2.1</td>
    </tr>
    <tr style="border-left:none; border-right: none; border-bottom:2px solid black">
        <td style="border-left:none; border-right: none">软件环境</td>
        <td style="border-left:none; border-right: none">python 3.8.13<br>PyTorch 1.10.1+cu111<br>flask 3.0.2<br>Ant Design 4.24.16<br>React 17</td>
        <td style="border-left:none; border-right: none">Google Chrome 122.0.6261.69（正式版本） (x86_64)</td>
    </tr>
</table>

用户可以通过浏览器输入指定网址访问图像和谐化系统，进入系统主页后可以看到用户登录页面，如图5-4所示。用户在输入用户名和密码后将跳转到和谐化系统页面。如果没有账号，可以点击右下角“用户注册”进行注册操作。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402291412079.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图5-4 系统登录界面
</p>



在用户成功登录系统后，系统将自动跳转到如下方图5-5所示的图像和谐化推理界面。推理界面主要包含传统图像和谐化模块以及本文所提出的多模态图像和谐化模块。在图像和谐化模块中，用户可以手动上传合成图片以及前景掩码，选择自己喜欢的和谐化模型，如默认的DiffHarmony模型，然后单击下方的“一键和谐”按钮，几秒钟后就能生成和谐化后的图像。如果用户预先没有获取到合成图像的前景掩码，那么可以使用下方的多模态图像和谐化模块。该模块需要用户手动输入合成图像以及一句文本提示，选择相关的模型，就能生成和谐化的图像。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402291443333.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图5-5 图像和谐化推理界面
</p>



在系统测试阶段，本文首先对图像和谐化模块进行了测试。如图5-6所示，输入一张合成图像以及前景掩码，选择本文所提出的DiffHarmony模型，点击“一键和谐”按钮后，系统自动生成了和谐化的图像。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402291501812.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图5-6 DiffHarmony模型系统推理结果
</p>

如图5-7所示，本文进一步对多模态图像和谐化模块进行了测试，上传合成图像并手动输入一句文本提示，选取分阶段的多模态和谐化模型CRIS+HT，系统自动生成了和谐化图片，效果符合预期。

<p style="text-align:center">
    <img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202402291517094.png" alt="image-20240115135228386" style="zoom:50%;" title="ceshi" /><br>
    图5-7 CRIS+HT模型系统推理结果
</p>



### 5.4 本章小结

本章主要介绍了图像和谐化系统的整体设计与实现，通过需求分析、系统设计、系统实现与测试较为详细的展示了此系统的构建流程。经过实验测试表明，该系统能够很好地满足用户的和谐化需求。



## 第六章 总结与展望



### 6.1 工作总结

图像和谐化任务作为图像合成领域中的一项重要子任务，具备很强的研究与应用价值。在深度学习不断发展的过程中，图像和谐化任务也取得了长足的进步，产生了广泛的市场应用价值。然而，当前的图像和谐化任务除了输入一张合成图片以外，还需要输入一张前景的掩码作为参照，这使得图像和谐化模型在很多没有预先获取到掩码的场景下无法发挥出作用。

为了消除和谐化对前景掩码的强依赖性，降低图像和谐化的实现门槛，本文首先从图文多模态的角度提出了一个新的和谐化解决方案，即多模态的图像和谐化。与需要输入掩码的传统和谐化任务不同，多模态图像和谐化通过输入合成图像和一句针对前景的指代性描述，就可以自动锁定前景区域并完成和谐化工作。相较于提供一张前景掩码图像，输入一句简单的文本提示要方便的多，进行和谐化操作的门槛也就相应降低。由于是一个新提出的多模态任务，当前并没有可直接应用于该任务训练的开源数据集，因此本文在传统图像和谐化任务中被广泛使用的iharmony4数据集的基础上，补充了针对其中所有合成图像前景区域的描述，构建出了一个新的多模态和谐化数据集ReiHarmony4。具体而言，本文首先通过3个指称表达生成模型和1个图像描述模型生成了4条与前景相关的文本描述，然后构建了一个专门的指称表达标注平台，并召集标注志愿者对生成的文本进行标注，评估生成文本的有效性，最后对标注结果进行检查和分析，过滤其中的错误信息，完成了新数据集的标注和构建操作。

针对这一新提出的多模态任务，本文提出了分割-和谐化的管道化模型以及端到端的两类模型。对于分割-和谐化的管道化模型，本文设计并实现了指称表达分割模块的CRIS模型与和谐化模块DHT、DiffHarmony的组合模型CRIS+HT、CRIS+DiffHarmony。在分割模块中通过直接输入合成图像而非真实图像，使模型能够在通过文本信息判断前景区域的同时还能通过前景背景本身的光照性差异进行辅助区分，提高了分割模块的分割能力。在和谐化模块中，本文创新性的提出了一种基于扩散模型的和谐化模型DiffHarmony，通过在经过预训练的稳定扩散模型的基础上进行微调获得了一个超越此前最佳性能的图像和谐化模型。对于端到端的多模态和谐化模型，本文实现了基于CRIS和DHT的模型CRISHT和基于DiffHarmony的端到端模型DiffReHarmony。其中，CRISHT通过加权损失对分阶段模型的两个模块进行了整合；DiffReHarmony在DiffHarmony模型的基础上通过调整输入、在U-Net网络层引入前景预测模块等4个方面的修改，完成了从分阶段模型到端到端模型的转变。实验结果表明，本文所提出的几个模型均能有效完成多模态图像和谐化任务。

此外，本文在上述研究的基础上，设计并实现了一个多模态图像和谐化系统。该系统允许用户以web页面的形式进行访问，通过输入一幅合成图像和一句针对于前景的指代性描述，自动生成和谐化后的图片，打通了从任务提出、数据集构建、模型设计、模型训练与调参到最终部署应用的全流程。



### 6.2 未来展望

本文在研究过程中提出了一个新的多模态图像和谐化任务，针对这一任务构建了一个新的数据集，设计了分阶段和端到端的几种不同模型，并将提出的模型成功部署到了实际的应用场景中。尽管取得了一定的阶段性成果，但是在研究过程中仍然还有一些问题有待进一步的探索，包括：

- 在分阶段模型的分割模块中，模型的分割性能还有待进一步的提升。特别是在复杂场景下，当前模型容易将目标对象和与其相同类别的其他对象混淆，导致错误的预测了前景区域，严重影响后续和谐化模块的效果。
- 本文最先提出了采用扩散模型来做图像和谐化任务的方法。然而，基于预训练的稳定扩散模型在进行和谐化的过程中会出现分辨率下降的问题，例如输入一张$256\times256$的图像，往往只能生成$32\times32$的和谐化结果，尽管可以通过提高输入图像的分辨率以及在模型输出后通过上采样的方式还原原始的分辨率，但这同样影响了最终的生成效果。
- 本文构建的ReiHarmony4数据集包含四个子数据集，数据规模相对较小，无法进行大规模的训练，这使得模型在处理一些特殊图像时的结果可能并不理想。如果能扩充数据集或是用自监督的方式进行训练可能将进一步提升模型性能。



## 参考文献

[1] <a name="ref-1" href="">Make Image Real Again</a>

[2] <a name="ref-2" href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Multi-scale+image+harmonization&btnG=">Multi-scale image harmonization</a>

[3] <a name="ref-3" href="https://arxiv.org/pdf/1505.04597.pdf%EF%BC%89">U-Net</a>

[4] <a name="ref-4" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cong_DoveNet_Deep_Image_Harmonization_via_Domain_Verification_CVPR_2020_paper.pdf">DoveNet</a>

[5] <a name="ref-5" href="">Lightness and Retinex theory</a>

[6] <a name="ref-6" href="">Intrinsic Image Harmonization</a>

[7] <a name="ref-7" href="">Understanding and improving the realism of image composites</a>

[8] <a name="ref-8" href="">Using color compatibility for assessing image realism</a>

[9] <a name="ref-9" href="">Learning a discriminative model for the perception of realism in composite images</a>

[10] <a name="ref-10" href="">Backpropagation Applied to Handwritten Zip Code Recognition</a>

[11] <a name="ref-11" href="">Color transfer between images</a>

[12] <a name="ref-12" href="">Deep image harmonization</a>

[13] <a name="ref-13" href="">DoveNet</a>

[14] <a name="ref-14" href="">Improving the harmony of the composite image by spatial-separated attention module</a>

[15] <a name="ref-15" href="">BargainNet</a>

[16] <a name="ref-16" href="">Harmony Transformer</a>

[17] <a name="ref-17" href="">Foreground-aware semantic representations for image harmonization</a>

[18] <a name="ref-18" href="">Region-aware Adaptive Instance Normalization for Image Harmonization</a>

[19] <a name="ref-19" href="">SSH: A Self-Supervised Framework for Image Harmonization</a>

[20] <a name="ref-20" href="">SCS-Co: Self-Consistent Style Contrastive Learning for Image Harmonization</a>

[21] <a name="ref-21" href="">Hierarchical Dynamic Image Harmonization</a>

[22] <a name="ref-22" href="">High-Resolution Image Harmonization via Collaborative Dual Transformations</a>

[23] <a name="ref-23" href="">Inharmonious region localization</a>

[24] <a name="ref-24" href="">Improving the harmony of the composite image by spatial-separated attention module</a>

[25] <a name="ref-25" href="">Exploring models and data for image question answering</a>

[26] <a name="ref-26" href="">LSTM</a>

[27] <a name="ref-27" href="">Ask your neurons: a neural-based approach to answering questions about images</a>

[28] <a name="ref-28" href="">Deep modular co-attention networks for visual question answering</a>

[29] <a name="ref-29" href="">Dynamic fusion with intra- and inter-modality attention flow for visual question answering</a>

[30] <a name="ref-30" href="">Bert: Pre-training of deep bidirectional transformers for language understanding</a>

[31] <a name="ref-31" href="">Learning Transferable Visual Models From Natural Language Supervision</a>

[32] <a name="ref-32" href="">Cris: Clip-driven referring image segmentation</a>

[33] <a name="ref-33" href="">Image style transfer with a single text condition</a>

[34] <a name="ref-34" href="">High-Resolution Image Synthesis with Latent Diffusion Models</a>

[35] <a name="ref-35" href="">A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY </a>

[36] <a name="ref-36" href="">Learning representations by back-propagating errors</a>

[37] <a name="ref-37" href="">Imagenet classification with deep convolutional neural networks</a>

[38] <a name="ref-38" href="">Going Deeper with Convolutions</a>

[39] <a name="ref-39" href="">Deep Residual Learning for Image Recognition</a>

[40] <a name="ref-40" href="">Gradient-based learning applied to document recognition</a>

[41] <a name="ref-41" href="">UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation</a>

[42] <a name="ref-42" href="">Generating Sequences With Recurrent Neural Networks</a>

[43] <a name="ref-43" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>

[44] <a name="ref-44" href="">Neural machine translation by jointly learning to align and translate</a>

[45] <a name="ref-45" href="">Attention Is All You Need</a>

[46] <a name="ref-46" href="">Language Models are Unsupervised Multitask Learners</a>

[47] <a name="ref-47" href="">Language Models are Few-Shot Learners</a>

[48] <a name="ref-48" href="">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a>

[49] <a name="ref-49" href="">Learning representations by back-propagating errors</a>

[50] <a name="ref-50" href="">Auto-Encoding Variational Bayes</a>

[51] <a name="ref-51" href="https://ldzhangyx.github.io/2018/11/20/vae/">VAE Intro</a>

[52] <a name="ref-52" href="https://zhuanlan.zhihu.com/p/33752313">GAN Intro</a>

[53] <a name="ref-53" href="">Generative adversarial networks</a>

[54] <a name="ref-54" href="">High-Resolution Image Synthesis with Latent Diffusion Models</a>

[55] <a name="ref-55" href="">Hierarchical Text-Conditional Image Generation with CLIP Latents</a>

[56] <a name="ref-56" href="">Denoising Diffusion Probabilistic Models</a>

[57] <a name="ref-57" href="">More Control for Free! Image Synthesis with Semantic Diffusion Guidance</a>

[58] <a name="ref-58" href="">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a>

[59] <a name="ref-59" href="">Improving Language Understanding by Generative Pre-Training</a>

[60] <a name="ref-60" href="">Training language models to follow instructions with human feedback</a>

[61] <a name="ref-61" href="">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a>

[62] <a name="ref-62" href="">ImageBERT</a>

[63] <a name="ref-63" href="">Oscar</a>

[64] <a name="ref-64" href="">CLIPstyler: Image Style Transfer with a Single Text Condition</a>

[65] <a name="ref-65" href="">Image Segmentation Using Text and Image Prompts</a>

[66] <a name="ref-66" href="">Referitgame: referring to objects in photographs of natural scenes</a>

[67] <a name="ref-67" href="">Generation and comprehension of unambiguous object descriptions</a>

[68] <a name="ref-68" href="">Towards unifying reference expression generation and comprehension</a>

[69] <a name="ref-69" href="">Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</a>

[70] <a name="ref-70" href="">An intriguing failing of convolutional neural networks and the CoordConv solution</a>

[71] <a name="ref-71" href="">Palette: image-to-image diffusion models</a>

[72] <a name="ref-72" href="">De-noising diffusion probabilistic models for robust image super-resolution in the wild</a>

[73] <a name="ref-73" href=""> Dpm-solver++: fast solver for guided sampling of diffusion probabilistic models</a>



## 附录







## 致谢







## 攻读学位期间取得的“创新成果”目录

[1] Referring image harmonization
