# **2022-10**

[toc]



## **周报**



### **1010 - 1017**



#### **- 10-11**

- **代码运行效率检查**

    ```shell
    dht 1 epoch 43min
    clip-dht 1 epoch  3h23min ......
    ```

    ```shell
    # 4 2080Ti GPU check
    ht       -- iters: 3500, time: 0.105, data: 18.206
    dht      -- iters: 800, time: 0.367, data: 33.265
    clip-ht  -- iters: 1300, time: 0.156, data: 86.695
    clip-dht -- iters: 800, time: 0.407, data: 90.498
     
    ```

- **`DHT checkpoint`**

    ```shell
    # 13 epoch
    # IHD MSE 74.68 | fMSE 534.41
    # HCOCO MSE 40.56 | fMSE 487.17
    
    # 35 epoch
    
    ```



### **多模态和谐化**



#### 过去工作

**多模态（基础）模型CLIP-HT性能相较于原始模型已经提升（提取的是图片的特征而非文本特征）**

​																								 **—— 表明CLIP抽取的多模态特征对和谐化工作是有帮助的**

<img src="https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202209210922890.png" style="zoom: 50%;" />

<table style="text-align:center">
    <caption>轻量模型CLIP-HT 训练指标</caption>
	<tr>
	    <th></th>
	    <th>HT(Prev)</th>
        <th>CLIP-HT</th>
	</tr>
	<tr>
	    <td>Parameters</td>
	    <td>4.773M</td>
        <td>160.241M</td>
	</tr>
	<tr>
	    <td>Dataset: MSE | fMSE</td>
	    <td>
            IHD: 37.07 | 395.66<br>
            Adobe: 47.96 | 321.14<br>
            COCO: 20.99 | 377.11<br>
            Flickr: 88.41 | 617.26
        </td>
        <td>
            IHD 33.75 | 374.05<br>
            Adobe 39.02 | 289.27<br>
            COCO 19.48 | 359.57<br>
            Flickr 84.53 | 594.32
        </td>
    </tr>
</table>




#### 当前工作

##### 设计用于和谐化的文本数据

##### 复杂一些的模型CLIP-DHT结果

<table style="text-align:center">
    <caption>CLIP-DHT 训练指标</caption>
	<tr>
	    <th></th>
	    <th>DHT(Prev)</th>
        <th>CLIP-DHT</th>
	</tr>
	<tr>
	    <td>Parameters</td>
	    <td>21.772M</td>
        <td>175.567M</td>
	</tr>
	<tr>
	    <td>Dataset: MSE | fMSE</td>
        <td>
            IHD: 30.30 | 320.78<br>
            Adobe: 38.53 | 265.11<br>
            COCO: 16.89 | 299.30<br>
            Flickr: 74.51 | 515.45
        </td>
        <td>
        	IHD: 31.10 | 337.91<br>
            Adobe: 39.05 | 273.32<br>
            COCO: 17.34 | 313.97<br>
            Flickr: 77.39 | 551.52
        </td>
	</tr>
</table>

##### 引入CLIP训练时间大幅上升的问题找到

![image-20221012195020835](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202210121950877.png)

**引入CLIP模型后，训练时长大幅提高问题找到，性能瓶颈在数据载入和预处理过程中（裁剪高像素图片，以及CLIP的预处理花费了比较多的时间），现在提前将图片裁剪成（256，256）后再作处理**



##### 新论文刷高了模型性能

**[【2022-09 TPAMI】Transformer for Image Harmonization and Beyond](https://ieeexplore.ieee.org/document/9893399)**

![image-20221012195801762](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202210121958827.png)

 



```shell
CUDA_VISIBLE_DEVICES=0 python test.py --model dht --name CNNDHT_2H9L_allihd --dataset_mode ihd --dataset_root /workspace/dataset/Image_Harmonization_Dataset/ --dataset_name ihd --batch_size 8

CUDA_VISIBLE_DEVICES=0 python test.py --model dht --name dht_v1.0 --dataset_mode ihd --dataset_root /workspace/dataset/Image_Harmonization_Dataset/ --dataset_name ihd --batch_size 8

CUDA_VISIBLE_DEVICES=0 python test.py --model mmht --name allihd_clip_v7 --dataset_mode mmihd --dataset_root /workspace/dataset/Image_Harmonization_Dataset/ --dataset_name HFlickr --batch_size 8
```



#### - 10-14

- **`CLIP-HT+`指标**

    <table style="text-align:center">
        <caption>轻量模型CLIP-HT+ 指标</caption>
    	<tr>
    	    <th></th>
    	    <th>HT+</th>
            <th>CLIP-HT+</th>
            <th>CLIP-HT+</th>
    	</tr>
    	<tr>
    	    <td>epoch</td>
    	    <td>60</td>
            <td>60</td>
            <td>90</td>
    	</tr>
    	<tr>
    	    <td>Dataset: MSE | fMSE</td>
    	    <td>
                IHD 30.46 | 341.01<br>
                Adobe 36.86 | 258.70<br>
                COCO 18.27 | 327.10<br>
                Flickr 73.87 | 561.89<br>
                Day2N 48.95 | 750.66
            </td>
            <td>
                IHD 31.88 | 368.17<br>
                Adobe 35.60 | 273.23<br>
                COCO 19.76 | 361.77<br>
                Flickr 82.34 | 579.87<br>
                Day2N 46.67 | 822.75
            </td>
            <td>
                IHD 31.17 | 359.50<br>
                Adobe 34.72 | 270.51<br>
                COCO 18.91 | 348.94<br>
                Flickr 82.81 | 581.62<br>
                Day2N 46.08 | 784.97
            </td>
        </tr>
    </table>



#### - 10-18

![clipres-ht](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202210181347929.png)

- **CLIPSeg**

    **[【2022 CVPR】Image Segmentation Using Text and Image Prompts](https://arxiv.org/pdf/2112.10003.pdf)**

    ![image-20221018135619276](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202210181356348.png)

    **三个问题**

    - **图片中存在多个相同种类物体时，很难确定具体是哪一个**

        ![image-20221018140429740](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202210181404786.png)

    - **只要文本中提到了某个物体，就会被标记出来（比如prompt：在水面上的动物；靠近卡车的电话亭。那么水面，以及卡车同样会被标记出来）**

    - **描述位置关系的prompt是无效的（比如prompt：画面左侧的沙发。那么“画面左侧”是无法被理解的）**

- **不和谐区域定位的新论文**

    **[【22.10.5 Arxiv】Inharmonious Region Localization with Auxiliary Style Feature](https://arxiv.org/pdf/2210.02029.pdf)**

    **[【22.10.5 Arxiv】Inharmonious Region Localization via Recurrent Self-Reasoning](https://arxiv.org/pdf/2210.02036.pdf)**

    



#### - 10-25

- **VGPhraseCut数据集**

    掩码部分被视作一个多边形，可以恢复成和谐化数据集的掩码图片

    70000+图片目前下载了20000+

    ![image-20221025092626977](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202210250926901.png)

    ![image-20221025093227328](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202210250932368.png)

- **图片的风格转换方式**

    - Dovenet论文提出的四种转换

        - global color transfer in Lαβ space
        -  global color transfer in RGB space
        - cumulative histogram matching
        - iterative color distribution transfer

        除了做变换，还进行了人工的过滤，把部分样本删除

    - SSH（自监督和谐化论文提出的使用3D-LUT的方式）

        ![image-20221025095653154](https://img-1300769438.cos.ap-beijing.myqcloud.com/images/202210250956203.png)

- **后续工作**

    - 整一个定时调度任务把图片全下载下来，下载的图片即作为ground-truth
    - 将图片语义保存为掩码图片的格式
    - 整理、下载100个3D-LUT .cube文件
    - 基于每个掩码，随机选取两个3D-LUT作风格变换得到合成图片（composite image）
    - 按照上述策略，最终的训练数据（图文对）会有60万+条，规模是此前iHarmony4数据集的十倍
    - 第一阶段的训练考虑冻结CLIPSeg模型权重，基于基础HT模型，训出一个和谐化模型
